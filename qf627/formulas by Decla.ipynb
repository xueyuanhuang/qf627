{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ecdf5e4",
   "metadata": {},
   "source": [
    "# Formulas\n",
    "## by Decla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ad88e",
   "metadata": {},
   "source": [
    "# Content（可直接 ctrl+F 相关索引）\n",
    "## import is important！\n",
    "## 抓取股票\n",
    "> 1（我承认这里有define function）\\\n",
    "> 2（但这里没有自己define的）\n",
    "## 双均线策略，以 20 天和 60 天为例\n",
    "## MDD\n",
    "## 最大回撤期\n",
    "## 设置一个rolling期（以41天为例）以及一个上下波动的阈值（以4为例）\n",
    ">1.单纯画图\\\n",
    ">2.画出买入卖出点\\\n",
    ">3.对比strategy return和原股票return（我认为大部分算return都可以按这个来）\n",
    "## sharpe ratio\n",
    "## Compound Annual Growth Rate，CAGR\n",
    "## mean-reversion: 抓取股票后由一个rolling 以及一个阈值（这里是两倍std）确定的上下限，制定策略\n",
    "> 1.画图\\\n",
    "> 2.计算sharpe ratio\n",
    "## 用之前的一定天数（以 3 天为例）预测明天的log return\n",
    "## MACD策略，长期(26天)短期(12天)差值与MACD signal(9天)\n",
    "## RSI \n",
    "## Combine MACD and RSI\n",
    "## PCA\n",
    "> group8\\\n",
    "> group9\n",
    "## Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d4e3a8",
   "metadata": {},
   "source": [
    "## <font color = Red> btw, if u think changing the name of dataframes is time-consuming, let's remind the function .copy() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e8a8e4",
   "metadata": {},
   "source": [
    "## import is important！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40948c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "import datetime\n",
    "import random\n",
    "import yfinance as yf\n",
    "import pandas_datareader.data as web \n",
    "\n",
    "(\n",
    "    yf\n",
    "    .pdr_override()\n",
    ")\n",
    "np.set_printoptions(precision = 3)\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "mpl.rcParams[\"axes.grid\"] = True\n",
    "mpl.rcParams[\"grid.color\"] = \"grey\"\n",
    "mpl.rcParams[\"grid.alpha\"] = 0.25\n",
    "\n",
    "mpl.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "\n",
    "mpl.rcParams[\"legend.fontsize\"] = 14\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from numpy.linalg import inv, eig, svd\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, AffinityPropagation, DBSCAN\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "from sklearn import cluster, covariance, manifold\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a2d133",
   "metadata": {},
   "source": [
    "## 抓取股票\n",
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec4da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks =\\\n",
    "    [\"WMT\", # Note that FB has been changed to META\n",
    "     \"IBM\"]\n",
    "def extract_sp(stocks, start, end):\n",
    "    def data(ticker):\n",
    "        return(pdr.get_data_yahoo(ticker,\n",
    "                                  start = start,\n",
    "                                  end = end)\n",
    "              )\n",
    "    FAANG_Stock = map(data, stocks)\n",
    "    return(pd.concat(FAANG_Stock,\n",
    "                     keys = stocks,\n",
    "                     names = [\"Tickers\", \"Date\"]\n",
    "                    )\n",
    "          )\n",
    "#设置起始和结束时间\n",
    "FAANG =\\\n",
    "    extract_sp(stocks,\n",
    "               datetime.datetime(2014, 10, 13),\n",
    "               datetime.datetime(2022, 9, 14)\n",
    "              )\n",
    "Daily_Closing_Price =\\\n",
    "(\n",
    "FAANG[[\"Adj Close\"]]\n",
    "    .reset_index()\n",
    "    .pivot(index = \"Date\",\n",
    "           columns = \"Tickers\",\n",
    "           values = \"Adj Close\")\n",
    ")\n",
    "Daily_Closing_Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f5d33f",
   "metadata": {},
   "source": [
    "### 2（这里没有定义函数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca891a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "google =\\\n",
    "(    web\n",
    "     .get_data_yahoo(\"GOOGL\", \n",
    "                     start, \n",
    "                     end)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919cf9d4",
   "metadata": {},
   "source": [
    "## 双均线策略，以 20 天和 60 天为例\n",
    ">注意只是画了红色的矩形图，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af50ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAANG2 =Daily_Closing_Price[\"IBM\"].copy()\n",
    "FAANG2 = pd.DataFrame(FAANG2)\n",
    "\n",
    "#更改天数在这里\n",
    "FAANG2[\"SMA_20\"] =\\\n",
    "(\n",
    "    FAANG2\n",
    "    [\"IBM\"]\n",
    "    .rolling(window = 20)\n",
    "    .mean()\n",
    ")\n",
    "FAANG2[\"SMA_60\"] =\\\n",
    "(\n",
    "    FAANG2\n",
    "    [\"IBM\"]\n",
    "    .rolling(60)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "#确定position\n",
    "FAANG2[\"positions\"] =\\\n",
    "(\n",
    "    np\n",
    "    .where(FAANG2[\"SMA_20\"] > FAANG2[\"SMA_60\"],\n",
    "           1, -1)\n",
    ")\n",
    "FAANG2 =\\\n",
    "(\n",
    "    FAANG2\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "ax =\\\n",
    "(\n",
    "    FAANG2\n",
    "    [[\"IBM\", \n",
    "      \"SMA_20\", \n",
    "      \"SMA_60\", \n",
    "      \"positions\"]]\n",
    "    .plot(secondary_y = \"positions\",\n",
    "          color = [\"grey\", \n",
    "                   \"blue\",\n",
    "                   \"green\",\n",
    "                   \"red\"],\n",
    "          style = [\"-\",\n",
    "                  \"--\",\n",
    "                  \"--\",\n",
    "                  \"-\"],\n",
    "          figsize = [18, 6]\n",
    "         )\n",
    ")\n",
    "\n",
    "(\n",
    "    ax\n",
    "    .legend(loc = \"upper center\",\n",
    "            shadow = True,\n",
    "            ncol = 4,\n",
    "            bbox_to_anchor = (0.55, 1.10),\n",
    "            fancybox = True\n",
    "           )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571adcfc",
   "metadata": {},
   "source": [
    "## MDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e087695",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAANG2[\"log_returns\"] =\\\n",
    "(\n",
    "    np\n",
    "    .log(FAANG2[\"Adj Close\"] / FAANG2[\"Adj Close\"].shift(1)\n",
    "        )\n",
    ")\n",
    "FAANG2[\"strategy_returns\"] =\\\n",
    "(\n",
    "    FAANG2[\"positions\"]\n",
    "    .shift(1)\n",
    "    *\n",
    "    FAANG2[\"log_returns\"] # passive following\n",
    ")\n",
    "FAANG2[\"cumulative_returns\"] =\\\n",
    "(\n",
    "    FAANG2[\"strategy_returns\"]\n",
    "    .cumsum()\n",
    "    .apply(np.exp)\n",
    ")\n",
    "FAANG2[\"max_gross_performance\"] =\\\n",
    "(\n",
    "    FAANG2\n",
    "    [\"cumulative_returns\"]\n",
    "    .cummax()\n",
    ")\n",
    "\n",
    "# (\n",
    "#     FAANG2\n",
    "#     [[\"cumulative_returns\", \"max_gross_performance\"]]\n",
    "#     .dropna()\n",
    "#     .plot(figsize = [18, 8]\n",
    "#          )\n",
    "# )\n",
    "FAANG2['drawdown'] = (FAANG2[\"max_gross_performance\"] - FAANG2[\"cumulative_returns\"])/FAANG2[\"max_gross_performance\"]\n",
    "FAANG2['drawdown'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3f5ca",
   "metadata": {},
   "source": [
    "## 最大回撤期\n",
    "## <font color = green> for instance, find the `five` worst drawdown periods over the investment horizon and provide their net drawdown in % and duration, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_drawdown = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            FAANG2[FAANG2.drawdown==0],# we need to get drawdown == 0\n",
    "            FAANG2[-1:] #plus last one\n",
    "        ],\n",
    "        axis=0\n",
    "    )\n",
    ")\n",
    "lst_day = []\n",
    "lst_drawdown = []\n",
    "for i in range(len(stock_drawdown) - 1):\n",
    "    days = (stock_drawdown.index[i+1] - stock_drawdown.index[i]).days\n",
    "    max_drawdown = FAANG2[stock_drawdown.index[i]:stock_drawdown.index[i+1]][\"drawdown\"].max()\n",
    "    lst_day.append(days)\n",
    "    lst_drawdown.append(max_drawdown)\n",
    "\n",
    "drawdown = pd.DataFrame([lst_day,lst_drawdown]).T\n",
    "drawdown.columns = [\"days\", \"drawdown\"]    \n",
    "drawdown.loc[drawdown[\"drawdown\"].nlargest(5).index]#这里改数字"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1207526",
   "metadata": {},
   "source": [
    "## 设置一个rolling期（以41天为例）以及一个上下波动的阈值（以4为例）\n",
    "### 1.单纯画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99fa6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAANG5 =Daily_Closing_Price[\"WMT\"].copy()\n",
    "FAANG5 = pd.DataFrame(FAANG5)\n",
    "\n",
    "#更改天数在这里\n",
    "FAANG5[\"SMA_41\"] =\\\n",
    "(\n",
    "    FAANG5[\"WMT\"]\n",
    "    .rolling(window = 41)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "#更改阈值在这里\n",
    "threshold = 4\n",
    "\n",
    "FAANG5[\"distance\"] = FAANG5[\"WMT\"] - FAANG5[\"SMA_41\"]\n",
    "\n",
    "(FAANG5[\"distance\"]\n",
    " .dropna()\n",
    " .plot(figsize = [16, 6]\n",
    "      )\n",
    ")\n",
    "\n",
    "plt.axhline(threshold, color = \"blue\", ls = \"--\")\n",
    "\n",
    "plt.axhline(0, color = \"green\", ls = \"--\")\n",
    "\n",
    "plt.axhline(-threshold, color = \"red\", ls = \"--\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c33e53c",
   "metadata": {},
   "source": [
    "### 2.画出买入卖出点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de312ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAANG6 =FAANG5.copy()\n",
    "FAANG6=FAANG6.reset_index()\n",
    "(\n",
    "FAANG5[\"distance\"]\n",
    ".dropna()\n",
    ".plot(figsize = [16, 7],\n",
    "      color = \"grey\",\n",
    "      alpha = 0.80)\n",
    ")\n",
    "\n",
    "# Upper-bound threshold\n",
    "\n",
    "plt.axhline(4,\n",
    "            color = \"blue\",\n",
    "            ls = \"--\")\n",
    "\n",
    "# Lower-bound threshold\n",
    "\n",
    "plt.axhline(-4,\n",
    "            color = \"blue\",\n",
    "            ls = \"--\")\n",
    "\n",
    "# Sell Signal\n",
    "\n",
    "FAANG5[\"positions\"] =\\\n",
    "(\n",
    "    np\n",
    "    .where(FAANG5[\"distance\"] > 4,\n",
    "           -1, np.nan)\n",
    ")\n",
    "\n",
    "# Buy Signal\n",
    "\n",
    "FAANG5[\"positions\"] =\\\n",
    "(\n",
    "    np\n",
    "    .where(FAANG5[\"distance\"] < -4,\n",
    "           1, FAANG5[\"positions\"])\n",
    ")\n",
    "\n",
    "# Market-Neutral Signal\n",
    "\n",
    "FAANG5[\"positions\"] =\\\n",
    "(\n",
    "    np\n",
    "    .where(FAANG5[\"distance\"] * FAANG5[\"distance\"].shift(1) < 0,\n",
    "           0, FAANG5[\"positions\"])\n",
    ")\n",
    "            \n",
    "sum_positions = 0\n",
    "\n",
    "for i in range(1994):\n",
    "    if FAANG5[\"positions\"][i] == 1:\n",
    "        plt.scatter(FAANG6['Date'][i],1,color='red',marker='.',s=100)\n",
    "        sum_positions  +=1\n",
    "    elif FAANG5[\"positions\"][i] == -1:\n",
    "        plt.scatter(FAANG6['Date'][i],-1,color='blue',marker='.',s=100)\n",
    "        sum_positions  +=1\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62ec00",
   "metadata": {},
   "source": [
    "### 3.对比strategy return和原股票return（我认为大部分算return都可以按这个来）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd37895",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAANG5[\"RETURNS\"] =\\\n",
    "(\n",
    "    np\n",
    "    .log(FAANG5[\"WMT\"]\n",
    "         /\n",
    "         FAANG5[\"WMT\"].shift(1)\n",
    "        )\n",
    ")\n",
    "FAANG5[\"positions\"] =\\\n",
    "    (FAANG5[\"positions\"]\n",
    "     .ffill()\n",
    "    )\n",
    "FAANG5[\"STRATEGY\"] =\\\n",
    "    (\n",
    "    FAANG5\n",
    "    [\"positions\"]\n",
    "    .shift(1)\n",
    "    *\n",
    "    FAANG5[\"RETURNS\"]\n",
    "    )\n",
    "(\n",
    "    FAANG5\n",
    "    [[\"RETURNS\", \"STRATEGY\"]]\n",
    "    .dropna()\n",
    "    .cumsum()\n",
    "    .apply(np.exp)\n",
    "    .plot(figsize = [18, 9]\n",
    "         )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48708600",
   "metadata": {},
   "source": [
    "## sharpe ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba74dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Portfolio[\"returns\"] = Portfolio[\"total\"].pct_change()\n",
    "\n",
    "Sharpe =np.sqrt(253) * (Portfolio[\"returns\"].mean() / Portfolio[\"returns\"].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76e5843",
   "metadata": {},
   "source": [
    "## Compound Annual Growth Rate，CAGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84459c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算交易日天数\n",
    "days_I =\\\n",
    "(    (FAANG2.index[-1] - FAANG2.index[0])\n",
    "     .days\n",
    ")\n",
    "\n",
    "FAANG2 =\\\n",
    "(\n",
    "    FAANG2\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "CAGR_I =\\\n",
    "(\n",
    "    (\n",
    "      (\n",
    "        (FAANG2[\"strategy_returns\"][-1]+1) / (FAANG2[\"strategy_returns\"][0]+1)\n",
    "      )\n",
    "        **(365.0/days_I) \n",
    "    ) \n",
    "    - 1\n",
    ")\n",
    "\n",
    "CAGR_I "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90efd269",
   "metadata": {},
   "source": [
    "## mean-reversion\n",
    "## 抓取股票后由一个rolling（42天） 以及一个阈值（这里是两倍std）确定的上下限，制定策略\n",
    "### 1.画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1f813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "google[\"Log_return\"] = np.log(google[\"Close\"]/google[\"Close\"].shift(1))\n",
    "\n",
    "#更改rolling天数在这一段！一段！一段！\n",
    "google[\"SMA_42\"] = google[\"Close\"].rolling(window=42).mean()\n",
    "google[\"Distance\"] = google[\"Close\"] - google[\"SMA_42\"]\n",
    "google[\"Upper_threshold\"] = 2*google[\"Close\"].rolling(window=42).std()\n",
    "google[\"Lower_threshold\"] = -2*google[\"Close\"].rolling(window=42).std()\n",
    "\n",
    "google[\"Trading_position\"] = np.where(google[\"Distance\"] > google[\"Upper_threshold\"], -1, np.nan)\n",
    "google[\"Trading_position\"] = np.where(google[\"Distance\"] < google[\"Lower_threshold\"], 1, google[\"Trading_position\"])\n",
    "google[\"Trading_position\"] = np.where(google[\"Distance\"]*google[\"Distance\"].shift(1) < 0, 0, google[\"Trading_position\"])\n",
    "google[\"Trading_position\"].ffill(inplace=True)\n",
    "\n",
    "google1 = google.copy()\n",
    "mask = (google1[\"Trading_position\"]!=google1[\"Trading_position\"].shift(1))\n",
    "google1[\"Signal\"] = google1[\"Trading_position\"].where(mask, 0)\n",
    "\n",
    "plt.figure(figsize=(18,9))\n",
    "plt.plot(google1[\"Close\"], label=\"Close\")\n",
    "plt.plot(google1[\"SMA_42\"], label=\"SMA_42\")\n",
    "plt.plot(google1[\"SMA_42\"]+google1[\"Upper_threshold\"], label=\"Upper_threshold\")\n",
    "plt.plot(google1[\"SMA_42\"]+google1[\"Lower_threshold\"], label=\"Lower_threshold\")\n",
    "plt.plot(google1[google1[\"Signal\"]==1][\"Close\"],\"g^\")\n",
    "plt.plot(google1[google1[\"Signal\"]==-1][\"Close\"],\"rv\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5589848f",
   "metadata": {},
   "source": [
    "### 2.计算Sharpe ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c829bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "google1[\"Log_strategy\"] = google1[\"Log_return\"]*google1[\"Trading_position\"].shift(1)\n",
    "google1[\"Strategy\"] = np.exp(google1[\"Log_strategy\"])-1\n",
    "sharpe_ratio = np.sqrt(253)*np.mean(google1[\"Strategy\"])/np.std(google1[\"Strategy\"])\n",
    "sharpe_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d782cee7",
   "metadata": {},
   "source": [
    "## 用之前的一定天数（以 3 天为例）预测明天的log return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b71df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ge = pdr.get_data_yahoo(\"GE\",dt.datetime(2011, 8, 1), dt.datetime(2020, 8, 1))\n",
    "ge[\"Log_return\"] = np.log(ge[\"Close\"]/ge[\"Close\"].shift(1))\n",
    "\n",
    "#更改天数在这里\n",
    "lags = 3\n",
    "cols = []\n",
    "for lag in range(1,lags+1):\n",
    "    col = f\"lag_{lag}\"\n",
    "    cols.append(col)\n",
    "    ge[col] = ge[\"Log_return\"].shift(lag)\n",
    "ge1 = ge.copy()\n",
    "ge1.dropna(inplace=True)\n",
    "OLS = np.linalg.lstsq(ge1[cols],ge1[\"Log_return\"])[0]\n",
    "ge1[\"Prediction\"] = np.dot(ge1[cols], OLS)\n",
    "\n",
    "accuracy = np.sign(ge1[\"Prediction\"]*ge1[\"Log_return\"]).value_counts()\n",
    "accuracy.values[0]/sum(accuracy)\n",
    "\n",
    "#画图\n",
    "# plt.figure(figsize=(18,9))\n",
    "# plt.plot(ge1[\"Prediction\"].cumsum(), label=\"Prediction_1_2_3\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "#计算Sharpe ratio\n",
    "#np.sqrt(253)*np.mean((np.exp(ge1[\"Prediction\"])-1))/np.std((np.exp(ge1[\"Prediction\"])-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a96f43",
   "metadata": {},
   "source": [
    "## MACD策略，长期(26天)短期(12天)差值与MACD signal(9天)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc8cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime(2015, 1, 1)\n",
    "end = datetime.datetime(2017, 12, 31)\n",
    "AAPL =\\\n",
    "(    web\n",
    "     .get_data_yahoo(\"AAPL\", \n",
    "                     start, \n",
    "                     end)\n",
    ")\n",
    "AAPL =AAPL.rename(columns={'Adj Close': 'Adj_Close'})\n",
    "AAPL[\"mean_12\"] =\\\n",
    "    (\n",
    "    AAPL[[\"Adj_Close\"]]\n",
    "    .rolling(12)\n",
    "    .mean()\n",
    "    )\n",
    "AAPL[\"mean_26\"] =\\\n",
    "    (\n",
    "    AAPL[[\"Adj_Close\"]]\n",
    "    .rolling(26)\n",
    "    .mean()\n",
    "    )\n",
    "AAPL[\"MACD\"]=AAPL[\"mean_12\"]-AAPL[\"mean_26\"]\n",
    "AAPL[\"MACD_signal\"] =\\\n",
    "    (\n",
    "    AAPL[[\"MACD\"]]\n",
    "    .rolling(9)\n",
    "    .mean()\n",
    "    )\n",
    "AAPL[\"Trading_Positions\"] =\\\n",
    "    (\n",
    "    np\n",
    "    .where(AAPL[\"MACD\"] > AAPL[\"MACD_signal\"],\n",
    "           1, -1)\n",
    "    )\n",
    "\n",
    "AAPL[\"Positions\"] =\\\n",
    "(    AAPL[\"Trading_Positions\"]\n",
    "     .diff()\n",
    ")\n",
    "\n",
    "AAPL = AAPL.reset_index()\n",
    "AAPL.set_index('Date', inplace=True)\n",
    "\n",
    "fig =\\\n",
    "    (plt\n",
    "     .figure(figsize = [16, 10]\n",
    "            )\n",
    "    )\n",
    "\n",
    "sub =\\\n",
    "(    fig\n",
    "    .add_subplot(111,\n",
    "                 ylabel = \"Stock Price\")\n",
    ")\n",
    "\n",
    "(AAPL[\"Adj_Close\"]\n",
    " .plot(ax = sub,\n",
    "       color = \"grey\",\n",
    "       lw = 0.80) # This is for closing price\n",
    ")\n",
    "\n",
    "(AAPL[[\"MACD_signal\", \n",
    "              \"MACD\"]]\n",
    "            .plot(ax = sub,\n",
    "                  style = [\"--\", \"--\"],\n",
    "                  lw = 0.80\n",
    "                 )\n",
    ")\n",
    "\n",
    "\n",
    "# Buy\n",
    "\n",
    "(\n",
    "    sub\n",
    "    .plot(AAPL.loc[AAPL['Positions'] == 2].index,#.astype(str),\n",
    "          AAPL.Adj_Close[AAPL['Positions'] == 2],\n",
    "          \"g^\",\n",
    "          color = \"green\",\n",
    "          markersize = 12)\n",
    "\n",
    ")\n",
    "\n",
    "# Sell\n",
    "\n",
    "(\n",
    "    sub\n",
    "    .plot(AAPL.loc[AAPL['Positions'] == -2].index,#,\n",
    "          AAPL.Adj_Close[AAPL['Positions'] == -2],\n",
    "          \"rv\",\n",
    "          color = \"red\",\n",
    "          markersize = 12)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3407dd",
   "metadata": {},
   "source": [
    "## RSI\n",
    "<left> $ RS = \\frac{AverageGain}{AverageLoss} $\n",
    "    \n",
    "<left> $ RSI = 100 - \\frac{100}{1 + RS} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb34a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = df['Adj Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "\n",
    "ag = gain.rolling(window=window_RS).mean()\n",
    "al = loss.rolling(window=window_RS).mean()\n",
    "\n",
    "rs = ag / al\n",
    "df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "df[\"position\"] = np.where(df['RSI'] > threshold_RSI, -1, 1)\n",
    "\n",
    "df[\"position\"] = df[\"position\"].shift(1)\n",
    "df[\"position\"].fillna(0, inplace=True)\n",
    "\n",
    "df[\"signal\"] = df[\"position\"].diff() \n",
    "df[\"signal\"].fillna(0, inplace=True)\n",
    "    \n",
    "df[\"signal\"]=np.sign(df[\"signal\"])\n",
    "\n",
    "# Create a figure and axis for the price chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot the price data\n",
    "ax.plot(df['Adj Close'], label='Price', color='b', linewidth=0.5)\n",
    "\n",
    "# Plot buy positions (green) and sell positions (red) based on the 'Signal' column\n",
    "buy_positions =df[df['signal']>0]\n",
    "sell_positions = df[df['signal'] <0]\n",
    "\n",
    "ax.scatter(buy_positions.index, buy_positions['Adj Close'], marker='^', color='g', label='Buy', s=10)\n",
    "ax.scatter(sell_positions.index, sell_positions['Adj Close'], marker='v', color='r', label='Sell', s=10)\n",
    "\n",
    "# Add labels and legend\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price')\n",
    "ax.set_title('RSI Buy and Sell Positions')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feda405",
   "metadata": {},
   "source": [
    "## Combine MACD and RSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136259c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "COM = pd.concat([MACD[['Adj Close', \"position\"]], RSI[[\"position\"]]], axis = 1) \n",
    "COM.columns = ['Adj Close', 'position1', 'position2'] \n",
    "COM['position'] = np.where(COM['position1']*COM['position2']>0, COM['position1'], 0)\n",
    "COM['signal'] = COM['position'].diff() \n",
    "COM['signal'].fillna(0, inplace=True)\n",
    "\n",
    "# Create a figure and axis for the price chart\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot the price data\n",
    "ax.plot(df['Adj Close'], label='Price', color='b', linewidth=0.5)\n",
    "\n",
    "# Plot buy positions (green) and sell positions (red) based on the 'Signal' column\n",
    "buy_positions =df[df['signal'] > 0]\n",
    "sell_positions = df[df['signal'] < 0]\n",
    "\n",
    "ax.scatter(buy_positions.index, buy_positions['Adj Close'], marker='^', color='g', label='Buy', s=10)\n",
    "ax.scatter(sell_positions.index, sell_positions['Adj Close'], marker='v', color='r', label='Sell', s=10)\n",
    "\n",
    "# Add labels and legend\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Price')\n",
    "ax.set_title('Combined Buy and Sell Positions')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d1889",
   "metadata": {},
   "source": [
    "## PCA\n",
    "### <font color = green> solution by group8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e30d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tickers = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\")[0]\n",
    "selected_symbols = random.sample(list_of_tickers['Symbol'].tolist(), 40)\n",
    "\n",
    "np.random.seed(627)\n",
    "selected_tickers = list_of_tickers.sample(n=40)\n",
    "data = []\n",
    "for ticker in selected_tickers['Symbol']:\n",
    "    stock_data = yf.download(ticker, start=\"2009-01-01\", end=\"2019-12-31\")\n",
    "    stock_data['Ticker'] = ticker\n",
    "    data.append(stock_data['Adj Close'])\n",
    "    \n",
    "data = pd.concat(data, axis=1)\n",
    "data.columns = selected_tickers['Symbol']\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "#Exploratory Data Analysis (EDA)\n",
    "#热力图\n",
    "corr = data.corr()\n",
    "plt.figure(figsize = [16, 16]\n",
    "          )\n",
    "plt.title(\"A Heatmap for Correlation Matrix\")\n",
    "sns \\\n",
    "    .heatmap(corr,\n",
    "             annot = True,\n",
    "             cmap = \"viridis\")\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "#Data Transformation\n",
    "missing_values =\\\n",
    "(\n",
    "    data\n",
    "    .isnull() # True (1) vs. False (0)\n",
    "    .mean()\n",
    "    .sort_values(ascending = False)\n",
    ")\n",
    "# missing_values.head(10)\n",
    "drop_list =\\\n",
    "(\n",
    "    sorted(list(missing_values[missing_values > 0.00]\n",
    "                .index)\n",
    "          )\n",
    ")\n",
    "stocks_df_cleaned =\\\n",
    "(\n",
    "    data\n",
    "    .drop(labels = drop_list,\n",
    "          axis = 1)\n",
    ")\n",
    "# print(data.shape)\n",
    "# print(stocks_df_cleaned.shape)\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "#Calculating Linear Daily Return\n",
    "Daily_Linear_Return =\\\n",
    "(\n",
    "    stocks_df_cleaned\n",
    "    .pct_change(1)\n",
    ")\n",
    "Daily_Linear_Return =\\\n",
    "(\n",
    "    Daily_Linear_Return[Daily_Linear_Return \n",
    "                        .apply(lambda x:(x - x.mean()\n",
    "                                        ).abs() < (3 * x.std()\n",
    "                                                  )\n",
    "                              )\n",
    "                        .all(1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "#Important considerations in data transformation for PCA\n",
    "scaler =\\\n",
    "(\n",
    "    StandardScaler()\n",
    "    .fit(Daily_Linear_Return)\n",
    ")\n",
    "scaled_df =\\\n",
    "(\n",
    "    pd\n",
    "    .DataFrame(scaler.fit_transform(Daily_Linear_Return),\n",
    "               columns = Daily_Linear_Return.columns,\n",
    "               index = Daily_Linear_Return.index)\n",
    ")\n",
    "plt.figure(figsize = [16, 6]\n",
    "          )\n",
    "\n",
    "plt.title(\"WYNN Return\")\n",
    "\n",
    "plt.ylabel(\"Linear Return\")\n",
    "\n",
    "(\n",
    "    scaled_df\n",
    "    [\"WYNN\"]\n",
    "    .plot()\n",
    ")\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "#Data split\n",
    "prop =\\\n",
    "    int(len(scaled_df) * 0.80)\n",
    "\n",
    "X_Train = scaled_df[    : prop] # First 80% of the data\n",
    "X_Test  = scaled_df[prop:     ] # Remaining 20% of the data\n",
    "\n",
    "X_Train_Raw = Daily_Linear_Return[    :prop]\n",
    "X_Test_Raw  = Daily_Linear_Return[prop:    ]\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "#Model Fitting\n",
    "pca = PCA()\n",
    "PrincipalComponent = pca.fit(X_Train)\n",
    "pca.components_[0]\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "#Explained Variance\n",
    "NumEigenValues = 10\n",
    "fig, axes =\\\n",
    "(\n",
    "    plt\n",
    "    .subplots(ncols = 2,\n",
    "              figsize = [16, 6]\n",
    "             )\n",
    ")\n",
    "# Plot on the left panel\n",
    "Series1 =\\\n",
    "(\n",
    "    pd\n",
    "    .Series(pca\n",
    "            .explained_variance_ratio_[ :NumEigenValues]\n",
    "           )\n",
    "    .sort_values()\n",
    "    * 100\n",
    ")\n",
    "# Plot on the right panel\n",
    "Series2 =\\\n",
    "(\n",
    "    pd\n",
    "    .Series(pca\n",
    "            .explained_variance_ratio_[ :NumEigenValues]\n",
    "           )\n",
    "    .cumsum()\n",
    "    * 100\n",
    ")\n",
    "(\n",
    "    Series1\n",
    "    .plot\n",
    "    .barh(ylim = (0, 9),\n",
    "          title = \"Explained Variance Ratio by Top 10\",\n",
    "          ax = axes[0]\n",
    "         )\n",
    ")\n",
    "(\n",
    "    Series2\n",
    "    .plot(ylim = (0, 100),\n",
    "          xlim = (0, 10),\n",
    "          title = \"Cumulative Explained Variance by Each PC\",\n",
    "          ax = axes[1]\n",
    "         )\n",
    ")\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "(\n",
    "    pd\n",
    "    .Series(np\n",
    "           .cumsum(pca\n",
    "                   .explained_variance_ratio_)\n",
    "           )\n",
    "    .to_frame(\"Explained Variance\")\n",
    "    .head(NumEigenValues)\n",
    "    .style\n",
    "    .format(\"{:,.2%}\".format)\n",
    ")\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "# Portfolio Weights\n",
    "pca.components_\n",
    "weights = pd.DataFrame()\n",
    "for i in range(len(pca.components_)):\n",
    "    weights[\"weights_{}\".format(i)] = pca.components_[i] / sum(pca.components_[i])\n",
    "    weights = weights.values.T\n",
    "# Set the number of principal components to be considered\n",
    "NumComponents = 4\n",
    "\n",
    "# Extract the top principal components from the PCA object\n",
    "# and create a DataFrame with columns named after the original features\n",
    "\n",
    "topPortfolios =\\\n",
    "(\n",
    "    pd\n",
    "    .DataFrame(pca.components_[ : NumComponents],\n",
    "               columns = stocks_df_cleaned.columns)\n",
    ")\n",
    "\n",
    "# Normalize the weights of the top portfolios such that the weights sum up to 1 for each portfolio\n",
    "# This is done by dividing each weight by the sum of weights for the respective portfolio\n",
    "\n",
    "eigen_portfolios =\\\n",
    "(\n",
    "    topPortfolios\n",
    "    .div(topPortfolios.sum(1),\n",
    "         axis = 0)\n",
    ")\n",
    "\n",
    "# Rename the index of the eigen_portfolios DataFrame for better readability\n",
    "\n",
    "eigen_portfolios.index = [f\"Portfolio {i}\" for i in range(NumComponents)]\n",
    "                         \n",
    "# Calculate the square root of the explained variance for each component\n",
    "# This provides the standard deviation of returns for each eigenportfolio\n",
    "np.sqrt(pca.explained_variance_)\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "eigen_portfolios\n",
    "                          \n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "(\n",
    "    eigen_portfolios\n",
    "    .T  # Transpose the DataFrame to have portfolios as columns and assets as rows\n",
    "    .plot\n",
    "    .bar(subplots = True,\n",
    "         layout = (int(NumComponents), 1),\n",
    "         legend = False,\n",
    "         sharey = True,\n",
    "         figsize = [16, 20],\n",
    "         ylim = [-1, 1]\n",
    "        )\n",
    ")\n",
    "\n",
    "plt.figure(figsize = [16, 6]\n",
    "          )\n",
    "\n",
    "sns.heatmap(topPortfolios,\n",
    "            cmap = \"viridis\")\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "#Best Eigen Portfolio\n",
    "def calculate_sharpe_ratio(ts_returns, periods_per_year = 252):\n",
    "\n",
    "    n_years = ts_returns.shape[0] / periods_per_year\n",
    "\n",
    "    annualized_return = np.power(np.prod(1 + ts_returns), (1 / n_years)\n",
    "                                ) - 1\n",
    "\n",
    "    annualized_vol = ts_returns.std() * np.sqrt(periods_per_year)\n",
    "\n",
    "    annualized_sharpe = annualized_return / annualized_vol\n",
    "\n",
    "    return annualized_return, annualized_vol, annualized_sharpe\n",
    "\n",
    "def recommend_optimal_portfolio():\n",
    "\n",
    "    # Number of eigenportfolios or principal components\n",
    "    \n",
    "    n_portfolios = len(pca.components_)\n",
    "\n",
    "    # Initialize arrays for annualized return, volatility, and Sharpe ratio of each eigenportfolio\n",
    "    \n",
    "    annualized_ret = np.array([0.] * n_portfolios)\n",
    "\n",
    "    sharpe_metric = np.array([0.] * n_portfolios)\n",
    "\n",
    "    annualized_vol = np.array([0.] * n_portfolios)\n",
    "\n",
    "    # Variable to track the index of the eigenportfolio with the highest Sharpe ratio\n",
    "    \n",
    "    highest_sharpe = 0\n",
    "\n",
    "    # Extract stock tickers from the scaled data\n",
    "    \n",
    "    stock_tickers =\\\n",
    "    (scaled_df\n",
    "     .columns \n",
    "     .values)\n",
    "\n",
    "    n_tickers = len(stock_tickers)\n",
    "\n",
    "    # Extract principal components\n",
    "    \n",
    "    PCs = pca.components_\n",
    "\n",
    "    # Loop through each eigenportfolio\n",
    "\n",
    "    for i in range(n_portfolios):\n",
    "\n",
    "        # Normalize the weights of the i-th eigenportfolio\n",
    "        \n",
    "        pc_w = PCs[i] / sum(PCs[i])\n",
    "\n",
    "        # Create a DataFrame for the eigenportfolio weights\n",
    "        \n",
    "        eigen_prtfi =\\\n",
    "            (\n",
    "                pd\n",
    "                .DataFrame(data = {\"weights\": pc_w.squeeze() * 100},\n",
    "                           index = stock_tickers)\n",
    "            )\n",
    "\n",
    "        # Calculate returns for the eigenportfolio\n",
    "        \n",
    "        eigen_prtfi.sort_values(by = [\"weights\"],\n",
    "                                ascending = False,\n",
    "                                inplace = True)\n",
    "\n",
    "        eigen_prti_returns =\\\n",
    "            (\n",
    "                np\n",
    "                .dot(X_Train_Raw.loc[ : , eigen_prtfi.index],\n",
    "                     pc_w)\n",
    "            )\n",
    "\n",
    "        eigen_prti_returns =\\\n",
    "            (\n",
    "                pd\n",
    "                .Series(eigen_prti_returns.squeeze(),\n",
    "                        index = X_Train_Raw.index)\n",
    "            )\n",
    "\n",
    "        # Calculate annualized return, volatility, and Sharpe ratio for the eigenportfolio\n",
    "        \n",
    "        er, vol, sharpe = calculate_sharpe_ratio(eigen_prti_returns)\n",
    "\n",
    "        # Store the metrics in their respective arrays\n",
    "        \n",
    "        annualized_ret[i] = er\n",
    "        annualized_vol[i] = vol\n",
    "        sharpe_metric[i] = sharpe\n",
    "\n",
    "        # Replace NaN values in Sharpe metric array with zeros\n",
    "        \n",
    "        sharpe_metric = np.nan_to_num(sharpe_metric)\n",
    "\n",
    "    # Let's find a portfolio with the HIGHEST Sharpe Ratio\n",
    "\n",
    "    highest_sharpe = np.argmax(sharpe_metric)\n",
    "\n",
    "    # Print the details of the eigenportfolio with the highest Sharpe ratio\n",
    "    \n",
    "    print(\"Our Eigen Portfolio #%d with the highest Sharpe\\\n",
    "           \\nReturn %.2f%%,\\vol = %.2f%%, \\nSharpe = %.2f\" %\n",
    "         (highest_sharpe,\n",
    "          annualized_ret[highest_sharpe] * 100,\n",
    "          annualized_vol[highest_sharpe] * 100,\n",
    "          sharpe_metric[highest_sharpe]\n",
    "         )\n",
    "         )\n",
    "\n",
    "    # Create a DataFrame to store the results for all eigenportfolios\n",
    "    \n",
    "    results =\\\n",
    "        (\n",
    "            pd\n",
    "            .DataFrame(data = {\"Return\": annualized_ret,\n",
    "                               \"Vol\": annualized_vol,\n",
    "                               \"Sharpe\": sharpe_metric}\n",
    "                      )\n",
    "        )\n",
    "\n",
    "    results.dropna(inplace = True)\n",
    "\n",
    "    results.sort_values(by = [\"Sharpe\"],\n",
    "                        ascending = False,\n",
    "                        inplace = True)\n",
    "\n",
    "    # Print the top 10 eigenportfolios based on Sharpe ratio\n",
    "\n",
    "    print(results.head(10)\n",
    "         )\n",
    "    \n",
    "def FindPortfolioVisual():\n",
    "    \n",
    "    n_portfolios = len(pca.components_)\n",
    "    \n",
    "    annualized_ret = np.array([0.] * n_portfolios)\n",
    "    \n",
    "    sharpe_metric = np.array([0.] * n_portfolios)\n",
    "    \n",
    "    annualized_vol = np.array([0.] * n_portfolios)\n",
    "    \n",
    "    highest_sharpe = 0\n",
    "    \n",
    "    stock_tickers = scaled_df.columns.values\n",
    "    \n",
    "    n_tickers = len(stock_tickers)\n",
    "    \n",
    "    PCs = pca.components_\n",
    "    \n",
    "    for i in range(n_portfolios):\n",
    "        \n",
    "        pc_w = PCs[i] / sum(PCs[i]\n",
    "                           )\n",
    "        \n",
    "        eigen_prtfi = pd.DataFrame(data = {\"weights\": pc_w.squeeze()*100}, \n",
    "                                   index = stock_tickers)\n",
    "        \n",
    "        eigen_prtfi.sort_values(by = [\"weights\"],\n",
    "                                ascending = False,\n",
    "                                inplace = True)\n",
    "        \n",
    "        eigen_prti_returns = np.dot(X_Train_Raw.loc[:, eigen_prtfi.index], \n",
    "                                    pc_w)\n",
    "        \n",
    "        eigen_prti_returns = pd.Series(eigen_prti_returns.squeeze(),\n",
    "                                       index = X_Train_Raw.index)\n",
    "        \n",
    "        er, vol, sharpe = calculate_sharpe_ratio(eigen_prti_returns)\n",
    "        \n",
    "        annualized_ret[i] = er\n",
    "        \n",
    "        annualized_vol[i] = vol\n",
    "        \n",
    "        sharpe_metric[i] = sharpe\n",
    "        \n",
    "        sharpe_metric = np.nan_to_num(sharpe_metric)\n",
    "        \n",
    "    # HOW TO FIND A PORTFOLIO with the HIGHEST Sharpe Ratio\n",
    "    \n",
    "    highest_sharpe = np.argmax(sharpe_metric)\n",
    "    \n",
    "    print(\"Our Eigen Portfolio #%d with the highest Sharpe. Return %.2f%%, vol = %.2f%%, Sharpe = %.2f\" %\n",
    "          (highest_sharpe,\n",
    "           annualized_ret[highest_sharpe]*100,\n",
    "           annualized_vol[highest_sharpe]*100,\n",
    "           sharpe_metric[highest_sharpe]\n",
    "          )\n",
    "         )\n",
    "        \n",
    "    #####\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    fig.set_size_inches(16, 6)\n",
    "    \n",
    "    ax.plot(sharpe_metric, \n",
    "            linewidth = 2)\n",
    "    \n",
    "    ax.set_title(\"Sharpe Ratio of Eigen-Portfolios\")\n",
    "    \n",
    "    ax.set_ylabel(\"Sharpe Ratio\")\n",
    "    \n",
    "    ax.set_xlabel(\"Portfolios\")\n",
    "    \n",
    "    #####\n",
    "        \n",
    "    results = pd.DataFrame(data = {\"Return\": annualized_ret, \"Vol\": annualized_vol, \"Sharpe\": sharpe_metric}\n",
    "                           )\n",
    "    \n",
    "    results.dropna(inplace = True)\n",
    "    \n",
    "    results.sort_values(by = [\"Sharpe\"],\n",
    "                        ascending = False,\n",
    "                        inplace = True)\n",
    "    \n",
    "    print(results.head(15)\n",
    "         )\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "FindPortfolioVisual()#这里可以看到sharpe ratio最高的三个weight序号\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "# Yet another gift\n",
    "\n",
    "def backtest_PCA_porfolios(eigen):\n",
    "\n",
    "    eigen_prtfi =\\\n",
    "        (\n",
    "            pd\n",
    "            .DataFrame(data = {\"weights\": eigen.squeeze()\n",
    "                              },\n",
    "                       index = stock_tickers)\n",
    "        )\n",
    "\n",
    "    eigen_prtfi.sort_values(by = [\"weights\"],\n",
    "                            ascending = False,\n",
    "                            inplace = True)\n",
    "\n",
    "    eigen_prtfi_returns =\\\n",
    "    (\n",
    "        np\n",
    "        .dot(X_Test_Raw\n",
    "             .loc[ : , eigen_prtfi.index],\n",
    "             eigen)\n",
    "    )\n",
    "\n",
    "    eigen_portfolio_returns =\\\n",
    "    (\n",
    "        pd\n",
    "        .Series(eigen_prtfi_returns.squeeze(),\n",
    "                index = X_Test_Raw.index)\n",
    "    )\n",
    "\n",
    "    returns, vol, sharpe = calculate_sharpe_ratio(eigen_portfolio_returns)\n",
    "\n",
    "    print(\"Our PCA-based Portfolio:\\nReturn = %.2f%%\\nVolatility = %.2f%%\\nSharpe = %.2f\"  %\n",
    "          (returns * 100, vol * 100, sharpe)\n",
    "         )\n",
    "\n",
    "    # Compared with what? Equal-weightage Portfolio\n",
    "\n",
    "    equal_weight_return =\\\n",
    "    (\n",
    "        X_Test_Raw * (1 / len(pca.components_)\n",
    "                     )\n",
    "    ).sum(axis = 1)\n",
    "\n",
    "    df_plot =\\\n",
    "        (\n",
    "            pd\n",
    "            .DataFrame({\"ML Portfolio Return\": eigen_portfolio_returns,\n",
    "                        \"Equal Weight Index\": equal_weight_return},\n",
    "                      index = X_Test.index\n",
    "                      )\n",
    "        )\n",
    "\n",
    "    (\n",
    "        np\n",
    "        .cumprod(df_plot + 1)\n",
    "        .plot(title = \"Returns of the equal weighted index vs. Eigen-Portfolio\",\n",
    "              figsize = [16, 8]\n",
    "             )\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "backtest_PCA_porfolios(eigen = weights[0]\n",
    "                      )\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "backtest_PCA_porfolios(eigen = weights[2]\n",
    "                      )\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "backtest_PCA_porfolios(eigen = weights[34]\n",
    "                      )\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03643d80",
   "metadata": {},
   "source": [
    "## PCA \n",
    "solution by group9\n",
    "### <font color = green> run a principal components analysis (PCA) for portfolio management. Begin your analysis with all the above stocks. Make sure to employ the inclusion criterion of less than 30% of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b8315",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq100_components = pd.read_html(\"https://en.wikipedia.org/wiki/Nasdaq-100\")[4]\n",
    "df1 = nasdaq100_components\n",
    "ls_symbol = df1['Ticker'].to_list()\n",
    "df = pdr.get_data_yahoo(ls_symbol, start = dt.datetime(2010,1,1), end = dt.datetime(2019,12,31))[['Adj Close']]\n",
    "missing_fractions = \\\n",
    "    df \\\n",
    "        .isnull() \\\n",
    "        .mean() \\\n",
    "        .sort_values(ascending = False)\n",
    "drop_list = \\\n",
    "    sorted(list(missing_fractions\n",
    "                [missing_fractions > 0.3]\n",
    "                .index)\n",
    "           )\n",
    "len(drop_list)# here we know that We have to delete 16 Tickers down till FANG\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "df = \\\n",
    "    df \\\n",
    "        .drop(labels= drop_list,\n",
    "              axis=1)\n",
    "df.shape[1] == 101 - 16\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "df = df.fillna(method = \"ffill\")\n",
    "# conduct EDA\n",
    "np.random.seed(231017)\n",
    "corr = df.corr()\n",
    "plt.figure(figsize = [20, 20])\n",
    "plt.title(\"A Heatmap for Correlation Matrix\")\n",
    "sns \\\n",
    "    .heatmap(corr,\n",
    "             annot = True,\n",
    "             cmap = \"viridis\")\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "\n",
    "Daily_Linear_Return = pd.DataFrame()\n",
    "Daily_Linear_Return = \\\n",
    "    (\n",
    "        df\n",
    "        .pct_change(1)\n",
    "    )\n",
    "Daily_Linear_Return = \\\n",
    "    (\n",
    "        Daily_Linear_Return[Daily_Linear_Return\n",
    "        .apply(lambda x:(x - x.mean()\n",
    "                         ).abs() < (4 * x.std()\n",
    "                                    )\n",
    "               )\n",
    "        .all(1)\n",
    "        ]\n",
    "    )\n",
    "print(\"No. of Outliers removed =\", df.shape[0] - Daily_Linear_Return.shape[0])\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "scaler = \\\n",
    "    (\n",
    "        StandardScaler()\n",
    "        .fit(Daily_Linear_Return)\n",
    "    )\n",
    "scaler\n",
    "\n",
    "#分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "scaled_df = \\\n",
    "    (\n",
    "        pd\n",
    "        .DataFrame(scaler.fit_transform(Daily_Linear_Return),\n",
    "                   columns = Daily_Linear_Return.columns,\n",
    "                   index = Daily_Linear_Return.index)\n",
    "    )\n",
    "\n",
    "prop = \\\n",
    "    int(len(scaled_df) * 0.75)\n",
    "\n",
    "X_Train = scaled_df[    : prop] # First 75% of the data\n",
    "X_Test  = scaled_df[prop:     ] # Remaining 25% of the data\n",
    "\n",
    "X_Train_Raw = Daily_Linear_Return[    :prop]\n",
    "X_Test_Raw  = Daily_Linear_Return[prop:    ]\n",
    "\n",
    "tickers = \\\n",
    "    (\n",
    "        scaled_df\n",
    "        .columns\n",
    "        .values\n",
    "    )\n",
    "\n",
    "pca = PCA()\n",
    "PCom = pca.fit(X_Train)\n",
    "PCom\n",
    "\n",
    "#分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "pca.components_\n",
    "\n",
    "#分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "EigenVal = 10\n",
    "fig, axes = \\\n",
    "    (\n",
    "        plt\n",
    "        .subplots(ncols = 2,\n",
    "                  figsize = [16, 6]\n",
    "                  )\n",
    "    )\n",
    "# Plot on the left panel\n",
    "Series1 = \\\n",
    "    (\n",
    "            pd\n",
    "            .Series(pca\n",
    "                    .explained_variance_ratio_[ :EigenVal]\n",
    "                    )\n",
    "            .sort_values()\n",
    "            * 100\n",
    "    )\n",
    "# Plot on the right panel\n",
    "Series2 = \\\n",
    "    (\n",
    "            pd\n",
    "            .Series(pca\n",
    "                    .explained_variance_ratio_[ :EigenVal]\n",
    "                    )\n",
    "            .cumsum()\n",
    "            * 100\n",
    "    )\n",
    "\n",
    "(\n",
    "    Series1\n",
    "    .plot\n",
    "    .barh(ylim = (0, 9),\n",
    "          title = \"Explained Variance Ratio by Top 10 PCs\",\n",
    "          ax = axes[0]\n",
    "          )\n",
    ")\n",
    "\n",
    "(\n",
    "    Series2\n",
    "    .plot(ylim = (0, 100),\n",
    "          xlim = (0, 9),\n",
    "          title = \"Cumulative Explained Variance by Each PC\",\n",
    "          ax = axes[1]\n",
    "          )\n",
    ")\n",
    "\n",
    "#分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "\n",
    "(\n",
    "    pd\n",
    "    .Series(np\n",
    "            .cumsum(pca\n",
    "                    .explained_variance_ratio_)\n",
    "            )\n",
    "    .to_frame(\"Explained Variance\")\n",
    "    .head(EigenVal)\n",
    "    .style\n",
    "    .format(\"{:,.2%}\".format)\n",
    ")\n",
    "\n",
    "\n",
    "weights = pd.DataFrame()\n",
    "for i in range(len(pca.components_)):\n",
    "    weights[\"weights_{}\".format(i)] = pca.components_[i] / sum(pca.components_[i])\n",
    "weights = weights.values.T\n",
    "weights\n",
    "\n",
    "\n",
    "weights[0]\n",
    "              \n",
    "    \n",
    "# Set the number of principal components at 4\n",
    "NumComp = 4\n",
    "# Extract the top 4 principal components from the PCA object\n",
    "# and create a DataFrame with columns named after the original features\n",
    "topPorts = \\\n",
    "    (\n",
    "        pd\n",
    "        .DataFrame(pca.components_[ : NumComp],\n",
    "                   columns = df.columns)\n",
    "    )\n",
    "# Normalize the weights of the top portfolios such that the weights sum up to 1 for each portfolio\n",
    "# This is done by dividing each weight by the sum of weights for the respective portfolio\n",
    "eigen_portfolios = \\\n",
    "    (\n",
    "        topPorts\n",
    "        .div(topPorts.sum(1),\n",
    "             axis = 0)\n",
    "    )\n",
    "# Rename the index of the eigen_portfolios DataFrame for better readability\n",
    "eigen_portfolios.index = [f\"Portfolio {i}\" for i in range(NumComp)\n",
    "                          ]\n",
    "# Calculate the square root of the explained variance for each component\n",
    "# This provides the standard deviation of returns for each eigenportfolio\n",
    "np.sqrt(pca.explained_variance_)\n",
    "\n",
    "\n",
    "## returns the eigen portfolios with the specified number of PrinComp\n",
    "# eigen_portfolios\n",
    "\n",
    "## return the first one\n",
    "# eigen_portfolios.iloc[0]\n",
    "\n",
    "(\n",
    "    eigen_portfolios\n",
    "    .T  # Transpose the DataFrame to have portfolios as columns and assets as rows\n",
    "    .plot\n",
    "    .bar(subplots = True,\n",
    "         layout = (int(NumComp), 1),\n",
    "         legend = False,\n",
    "         sharey = True,\n",
    "         figsize = [16, 20],\n",
    "         ylim = [-1, 1]\n",
    "         )\n",
    ")\n",
    "\n",
    "\n",
    "## Eigenportfolio#0 is not properly plotted due to lower y-values, replot using different scale\n",
    "(\n",
    "    eigen_portfolios.iloc[0]\n",
    "    .T  # Transpose the DataFrame to have portfolios as columns and assets as rows\n",
    "    .plot\n",
    "    .bar(subplots = True,\n",
    "         layout = (int(NumComp), 1),\n",
    "         legend = False,\n",
    "         sharey = True,\n",
    "         figsize = [16, 20],\n",
    "         ylim = [-0.02, 0.02]\n",
    "         )\n",
    ")\n",
    "\n",
    "\n",
    "plt.figure(figsize=[16, 6]\n",
    "           )\n",
    "sns.heatmap(topPorts,\n",
    "            cmap = \"viridis\")\n",
    "\n",
    "\n",
    "n_ports = len(pca.components_)\n",
    " # Initialize arrays for annualized return, volatility, and Sharpe ratio of each eigenportfolio\n",
    "annualized_ret = np.array([0.] * n_ports)\n",
    "sharpe_metric = np.array([0.] * n_ports)\n",
    "annualized_vol = np.array([0.] * n_ports)\n",
    "# Variable to track the index of the eigenportfolio with the highest Sharpe ratio, initialise before each run = 0\n",
    "highest_sharpe = 0\n",
    "# Extract stock tickers from the scaled data\n",
    "stock_tickers = \\\n",
    "    (scaled_df\n",
    "     .columns\n",
    "     .values)\n",
    "stock_tickers\n",
    "\n",
    "\n",
    "annualized_ret\n",
    "\n",
    "\n",
    "PCs = pca.components_\n",
    "pc_w = PCs[84]/sum(PCs[84])\n",
    "np.sum(pc_w)\n",
    "\n",
    "\n",
    "pc_w.squeeze()*100\n",
    "\n",
    "\n",
    "eigen_prtfi = \\\n",
    "    (\n",
    "        pd\n",
    "        .DataFrame(data={\"weights\": pc_w.squeeze() * 100},\n",
    "                   index=stock_tickers)\n",
    "    )\n",
    "eigen_prtfi\n",
    "## negative weights = short, positive weights = long\n",
    "\n",
    "\n",
    "eigen_prtfi.sort_values(by = [\"weights\"],\n",
    "                        ascending = False,\n",
    "                        inplace = True)\n",
    "eigen_prtfi\n",
    "### GOOGL features No.1 in weights from among the PCAComp. But it is also the last weight. Why is that? One is Class A and one is Class C (non-voting shares), The net is amount right, +05\n",
    "\n",
    "\n",
    "X_Train_Raw.loc[ : , eigen_prtfi.index]\n",
    "## slightly less than 4 years worth of complete data points on top PrinComp starting from left\n",
    "\n",
    "\n",
    "## Generate returns for the Train Set\n",
    "eigen_prti_returns = \\\n",
    "    (\n",
    "        np\n",
    "        .dot(X_Train_Raw.loc[ : , eigen_prtfi.index],\n",
    "             pc_w)\n",
    "    )\n",
    "\n",
    "\n",
    "periods_per_year = 252\n",
    "n_years = eigen_prti_returns.shape[0] / periods_per_year\n",
    "n_years\n",
    "\n",
    "\n",
    "np.prod(1+eigen_prti_returns)\n",
    "\n",
    "\n",
    "annualized_return = np.power(np.prod(1 + eigen_prti_returns), (1 / n_years)\n",
    "                             ) - 1\n",
    "\n",
    "\n",
    "## Build function for calculating Sharpe Ratio on Train set\n",
    "def calculate_sharpe_ratio(ts_returns, periods_per_year = 252):\n",
    "\n",
    "    n_years = ts_returns.shape[0] / periods_per_year\n",
    "\n",
    "    annualized_return = np.power(np.prod(1 + ts_returns), (1 / n_years)\n",
    "                                 ) -   1\n",
    "\n",
    "    annualized_vol = ts_returns.std() * np.sqrt(periods_per_year)\n",
    "\n",
    "    annualized_sharpe = annualized_return / annualized_vol\n",
    "\n",
    "    return annualized_return, annualized_vol, annualized_sharpe\n",
    "# Build a function to recommend Top 4 best eigen portfolios/Top 4 PrinComp\n",
    "def recommend_optimal_portfolio():\n",
    "    # Number of eigenportfolios or principal components\n",
    "    n_portfolios = len(pca.components_[:4])\n",
    "    # Initialize arrays for annualized return, volatility, and Sharpe ratio of each eigenportfolio\n",
    "    annualized_ret = np.array([0.] * n_portfolios)\n",
    "    sharpe_metric = np.array([0.] * n_portfolios)\n",
    "    annualized_vol = np.array([0.] * n_portfolios)\n",
    "    # Variable to track the index of the eigenportfolio with the highest Sharpe ratio\n",
    "    highest_sharpe = 0\n",
    "    # Extract stock tickers from the scaled data\n",
    "    stock_tickers = \\\n",
    "        (scaled_df\n",
    "         .columns\n",
    "         .values)\n",
    "    n_tickers = len(stock_tickers)\n",
    "    # Extract principal components\n",
    "    PCs = pca.components_[:4]\n",
    "    # Loop through each eigenportfolio\n",
    "    for i in range(n_portfolios):\n",
    "        # Normalize the weights of the i-th eigenportfolio\n",
    "        pc_w = PCs[i] / sum(PCs[i])\n",
    "        # Create a DataFrame for the eigenportfolio weights\n",
    "        eigen_prtfi = \\\n",
    "            (\n",
    "                pd\n",
    "                .DataFrame(data = {\"weights\": pc_w.squeeze() * 100},\n",
    "                           index = stock_tickers)\n",
    "            )\n",
    "        # Calculate returns for the eigenportfolio\n",
    "        eigen_prtfi.sort_values(by = [\"weights\"],\n",
    "                                ascending = False,\n",
    "                                inplace = True)\n",
    "        eigen_prti_returns = \\\n",
    "            (\n",
    "                np\n",
    "                .dot(X_Train_Raw.loc[ : , eigen_prtfi.index],\n",
    "                     pc_w)\n",
    "            )\n",
    "        eigen_prti_returns = \\\n",
    "            (\n",
    "                pd\n",
    "                .Series(eigen_prti_returns.squeeze(),\n",
    "                        index = X_Train_Raw.index)\n",
    "            )\n",
    "        # Calculate annualized return, volatility, and Sharpe ratio for the eigenportfolio\n",
    "        er, vol, sharpe = calculate_sharpe_ratio(eigen_prti_returns)\n",
    "        # Store the metrics in their respective arrays\n",
    "        annualized_ret[i] = er\n",
    "        annualized_vol[i] = vol\n",
    "        sharpe_metric[i] = sharpe\n",
    "        # Replace NaN values in Sharpe metric array with zeros\n",
    "        sharpe_metric = np.nan_to_num(sharpe_metric)\n",
    "    # Let's find a portfolio with the HIGHEST Sharpe Ratio\n",
    "    highest_sharpe = np.argmax(sharpe_metric)\n",
    "    # Print the details of the eigenportfolio with the highest Sharpe ratio\n",
    "    print(\"Our Eigen Portfolio #%d with the highest Sharpe\\\n",
    "           \\nReturn %.2f%%,\\vol = %.2f%%, \\nSharpe = %.2f\" %\n",
    "          (highest_sharpe,\n",
    "           annualized_ret[highest_sharpe] * 100,\n",
    "           annualized_vol[highest_sharpe] * 100,\n",
    "           sharpe_metric[highest_sharpe]\n",
    "           ))\n",
    "    # Create a DataFrame to store the results for all eigenportfolios\n",
    "    results = \\\n",
    "        (\n",
    "            pd\n",
    "            .DataFrame(data = {\"Return\": annualized_ret,\n",
    "                               \"Vol\": annualized_vol,\n",
    "                               \"Sharpe\": sharpe_metric}\n",
    "                       ))\n",
    "    results.dropna(inplace = True)\n",
    "    results.sort_values(by = [\"Sharpe\"],\n",
    "                        ascending = False,\n",
    "                        inplace = True)\n",
    "    # Print the top 10 eigenportfolios based on Sharpe ratio\n",
    "    print(results.head(10)\n",
    "          )\n",
    "    return \n",
    "\n",
    "\n",
    "recommend_optimal_portfolio()\n",
    "\n",
    "\n",
    "top = 10\n",
    "n_portfolios = len(pca.components_[:top])\n",
    "annualized_ret = np.array([0.] * n_portfolios)\n",
    "sharpe_metric = np.array([0.] * n_portfolios)\n",
    "annualized_vol = np.array([0.] * n_portfolios)\n",
    "highest_sharpe = 0\n",
    "stock_tickers = scaled_df.columns.values\n",
    "n_tickers = len(stock_tickers)\n",
    "PCs = pca.components_[:top]\n",
    "for i in range(n_portfolios):\n",
    "    pc_w = PCs[i] / sum(PCs[i])\n",
    "    eigen_prtfi = pd.DataFrame(data = {\"weights\": pc_w.squeeze()*100},\n",
    "                                index = stock_tickers)\n",
    "    eigen_prtfi.sort_values(by = [\"weights\"],\n",
    "                            ascending = False,\n",
    "                            inplace = True)\n",
    "    eigen_prti_returns = np.dot(X_Train_Raw.loc[:, eigen_prtfi.index],\n",
    "                                pc_w)\n",
    "    eigen_prti_returns = pd.Series(eigen_prti_returns.squeeze(),\n",
    "                                    index = X_Train_Raw.index)\n",
    "    er, vol, sharpe = calculate_sharpe_ratio(eigen_prti_returns)\n",
    "    annualized_ret[i] = er\n",
    "    annualized_vol[i] = vol\n",
    "    sharpe_metric[i] = sharpe\n",
    "    sharpe_metric = np.nan_to_num(sharpe_metric)\n",
    "# HOW TO FIND A PORTFOLIO with the HIGHEST Sharpe Ratio\n",
    "highest_sharpe = np.argmax(sharpe_metric)\n",
    "print(\"From Train Set, the Eigen Portfolio #%d with the highest Sharpe. Return %.2f%%, vol = %.2f%%, Sharpe = %.2f\" %\n",
    "        (highest_sharpe,\n",
    "        annualized_ret[highest_sharpe]*100,\n",
    "        annualized_vol[highest_sharpe]*100,\n",
    "        sharpe_metric[highest_sharpe]\n",
    "        ))\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 6)\n",
    "ax.plot(sharpe_metric,linewidth = 2)\n",
    "ax.set_title(\"Sharpe Ratio of Eigen-Portfolios\")\n",
    "ax.set_ylabel(\"Sharpe Ratio\")\n",
    "ax.set_xlabel(\"Portfolios\")\n",
    "results = pd.DataFrame(data = {\"Return\": annualized_ret, \"Vol\": annualized_vol, \"Sharpe\": sharpe_metric})\n",
    "results.dropna(inplace = True)\n",
    "results.sort_values(by = [\"Sharpe\"],\n",
    "                        ascending = False,\n",
    "                        inplace = True)\n",
    "print(results.head(15))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Portfolio #0\")\n",
    "eigen=weights[0]\n",
    "eigen_prtfi =pd.DataFrame(data={\"weights\": eigen.squeeze()},\n",
    "                       index=stock_tickers)\n",
    "eigen_prtfi.sort_values(by=[\"weights\"],\n",
    "                        ascending=False,\n",
    "                        inplace=True)\n",
    "eigen_prtfi_returns = np.dot(X_Test_Raw\n",
    "                             .loc[:, eigen_prtfi.index],\n",
    "                             eigen)\n",
    "eigen_portfolio_returns = pd.Series(eigen_prtfi_returns.squeeze(),\n",
    "                    index=X_Test_Raw.index)\n",
    "returns, vol, sharpe = calculate_sharpe_ratio(eigen_portfolio_returns)\n",
    "print(\"From Test Set, Our PCA-based Portfolio performed as follows:\\nReturn = %.2f%%\\nVolatility = %.2f%%\\nSharpe = %.2f\" %\n",
    "    (returns * 100, vol * 100, sharpe))\n",
    "# Compared with what? Equal-weightage Portfolio\n",
    "equal_weight_return = X_Test_Raw * (1 / len(pca.components_)).sum(axis=1)\n",
    "df_plot = pd.DataFrame({\"ML Portfolio Return\": eigen_portfolio_returns,\n",
    "                        \"Equal Weight Index\": equal_weight_return},\n",
    "                       index=X_Test.index\n",
    "                       )\n",
    "(\n",
    "    np\n",
    "    .cumprod(df_plot + 1)\n",
    "    .plot(title=\"Returns of the equal weighted index vs. Eigen-Portfolio\",\n",
    "            figsize=[16, 8]\n",
    "            )\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aecf23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Portfolio #5\")\n",
    "eigen=weights[6]\n",
    "eigen_prtfi =pd.DataFrame(data={\"weights\": eigen.squeeze()},\n",
    "                       index=stock_tickers)\n",
    "eigen_prtfi.sort_values(by=[\"weights\"],\n",
    "                        ascending=False,\n",
    "                        inplace=True)\n",
    "eigen_prtfi_returns = np.dot(X_Test_Raw\n",
    "                             .loc[:, eigen_prtfi.index],\n",
    "                             eigen)\n",
    "eigen_portfolio_returns = pd.Series(eigen_prtfi_returns.squeeze(),\n",
    "                    index=X_Test_Raw.index)\n",
    "returns, vol, sharpe = calculate_sharpe_ratio(eigen_portfolio_returns)\n",
    "print(\"From Test Set, Our PCA-based Portfolio performed as follows:\\nReturn = %.2f%%\\nVolatility = %.2f%%\\nSharpe = %.2f\" %\n",
    "    (returns * 100, vol * 100, sharpe))\n",
    "# Compared with what? Equal-weightage Portfolio\n",
    "equal_weight_return = X_Test_Raw * (1 / len(pca.components_)).sum(axis=1)\n",
    "df_plot = pd.DataFrame({\"ML Portfolio Return\": eigen_portfolio_returns,\n",
    "                        \"Equal Weight Index\": equal_weight_return},\n",
    "                       index=X_Test.index\n",
    "                       )\n",
    "(\n",
    "    np\n",
    "    .cumprod(df_plot + 1)\n",
    "    .plot(title=\"Returns of the equal weighted index vs. Eigen-Portfolio\",\n",
    "            figsize=[16, 8]\n",
    "            )\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b30f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    eigen_portfolios.iloc[0]\n",
    "    .T  # Transpose the DataFrame to have portfolios as columns and assets as rows\n",
    "    .plot\n",
    "    .bar(subplots = True,\n",
    "         layout = (int(NumComp), 1),\n",
    "         legend = False,\n",
    "         sharey = True,\n",
    "         figsize = [16, 20],\n",
    "         ylim = [-0.02, 0.02]\n",
    "         )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7675c0",
   "metadata": {},
   "source": [
    "### <font color = blue> 可恶多剩下的不想搞了，直接看group9的set吧"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02095de1",
   "metadata": {},
   "source": [
    "## Cluster\n",
    "### <font color = green> Using the 102 tickers below, and what you have learned in class, run the analysis and develop a dendrogram. Make sure to employ the inclusion criterion of less than 30% of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df376a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq100_components = pd.read_html(\n",
    "    \"https://en.wikipedia.org/wiki/Nasdaq-100\")[4]\n",
    "TICKERS = nasdaq100_components['Ticker'].tolist()\n",
    "\n",
    "# here to change the period\n",
    "start = datetime.date(2019, 10, 24)\n",
    "end = datetime.date(2023, 10, 25)\n",
    "\n",
    "stock_data = []\n",
    "for ticker in TICKERS:\n",
    "    stock_data.append(pdr.get_data_yahoo(TICKERS, start, end))\n",
    "nasdaq100_stocks = pd.concat(stock_data, keys=TICKERS, names=[\"ticker\", \"date\"])\n",
    "nasdaq100_closing_prices =\\\n",
    "    (\n",
    "        nasdaq100_stocks[[\"Adj Close\"]]\n",
    "        .reset_index()\n",
    "    )\n",
    "nasdaq100_closing_prices_WIDE =\\\n",
    "    (\n",
    "        nasdaq100_closing_prices\n",
    "        .pivot(index=\"date\",\n",
    "               columns=\"ticker\",\n",
    "               values=\"Adj Close\")\n",
    "    )\n",
    "# show the proportion of missing values in the entire stock column\n",
    "missing_fractions = \\\n",
    "    nasdaq100_closing_prices_WIDE \\\n",
    "    .isnull() \\\n",
    "    .mean() \\\n",
    "    .sort_values(ascending=False)\n",
    "missing_fractions# here we have three columns which have more than 30% missing values\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "# We create a list containing the names of these three columns.\n",
    "drop_list =\\\n",
    "    sorted(list(missing_fractions\n",
    "                [missing_fractions > 0.3]\n",
    "                .index)\n",
    "           )\n",
    "# Then we drop them.\n",
    "nasdaq100_closing_prices_WIDE =\\\n",
    "    nasdaq100_closing_prices_WIDE \\\n",
    "    .drop(labels=drop_list,\n",
    "          axis=1)\n",
    "nasdaq100_closing_prices_WIDE.shape[1] == 101 - 3# as we have 101 columns in total, and three columns which have more than 30% missing values\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "nasdaq100_closing_prices_WIDE = nasdaq100_closing_prices_WIDE.fillna(\n",
    "    method=\"ffill\")\n",
    "returns = nasdaq100_closing_prices_WIDE.pct_change().dropna()\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, ward\n",
    "returns_corr = returns.corr()# correlation between columns\n",
    "dist = ((1 - returns_corr) / 2.0) ** 0.5\n",
    "# Then we can easily calculate the linkage, which is the basis for us to draw Dendograms.\n",
    "link =linkage(dist,\"ward\")\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.title(\"Stocks Dendograms\")\n",
    "dendrogram(link, labels=dist.index)\n",
    "plt.show()\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "link[0]\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "link_up = link[np.argsort(link[:, 2])]\n",
    "link_up[0]# here we get the reault 41 and 42\n",
    "#分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "stock_mapping = {index: stock_name for\n",
    "                 index,stock_name\n",
    "                 in enumerate(dist.index)}\n",
    "print(\n",
    "    f\"{stock_mapping[41]} and {stock_mapping[42]} is the most corelated pair of stocks.\")\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "link_down = link_stocks[np.argsort(link_stocks[:, 2])[::-1]]\n",
    "link_down[0]# here we get the reault 51 and 83\n",
    "#分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "print(f\"{stock_mapping[51]} and {stock_mapping[83]} is the least correlated pair of stocks.\")\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.title(\"Stocks Dendograms\")\n",
    "dendrogram(link, labels=dist.index)\n",
    "plt.show()\n",
    "\n",
    "##分段---------------------------------------------------------------------------------------------------------------------------------##\n",
    "print(f\"{stock_mapping[41]} and {stock_mapping[42]} is the most corelated pair of stocks.\")\n",
    "print(f\"{stock_mapping[51]} and {stock_mapping[83]} is the least correlated pair of stocks.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
