{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QF 627 Programming and Computational Finance\n",
    "## Problem-Sets for Exercise `9` | `Questions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Hi Team, üëã\n",
    "\n",
    "> The initial problem sets were designed for practicing supervised learning in classification problems and hierarchical risk parity algorithms, as well as applying unsupervised learning to portfolio management.\n",
    "\n",
    "> Given that we haven't covered some these topics in depth yet and will be discussing them further in Lessons 9 and 10, the problem sets have been revised.\n",
    "\n",
    "> Having reviewed your submissions so far, some of the questions have been crafted specifically to enhance your grasp of the course material.\n",
    "\n",
    "> I trust that the exercises below will support your review and understanding of the course content. ü§û\n",
    "\n",
    "#### <font color = \"green\"> Please submit your answers via the eLearn submission folder. Again, you may submit incomplete answers. (Answer as fully as you can. This will help me to see where you stand.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For standardization of your answers‚Ä¶\n",
    "\n",
    "> Please execute the lines of code below before you start work on your answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from pandas_datareader import data as pdr\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "yf.pdr_override()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's set some print option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision = 3)\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "mpl.rcParams[\"axes.grid\"] = True\n",
    "mpl.rcParams[\"grid.color\"] = \"grey\"\n",
    "mpl.rcParams[\"grid.alpha\"] = 0.25\n",
    "\n",
    "mpl.rcParams[\"axes.facecolor\"] = \"white\"\n",
    "\n",
    "mpl.rcParams[\"legend.fontsize\"] = 14\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëá <font color = \"purple\"> Bigger Question 1. \n",
    "    \n",
    "### The first question is to look for clusters of correlations using the agglomerate hierarchical clustering technique (AGNES).\n",
    "    \n",
    "### <font color = green> Using the 102 tickers below, and what you have learned in class, run the analysis and develop a dendrogram. Make sure to employ the inclusion criterion of less than 30% of missing values.\n",
    "    \n",
    "    According to the dendrogram, which of the stocks are most correlated? \n",
    "    \n",
    "    Also based on the dendrogram, please identify two stocks that are not well correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasdaq100_components = pd.read_html(\"https://en.wikipedia.org/wiki/Nasdaq-100\")[4]\n",
    "\n",
    "nasdaq100_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the lines of code that lead to an answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = pdr.get_data_yahoo(list(nasdaq100_components[\"Ticker\"]), start=dt.datetime(2000,1,1))\n",
    "stocks = stocks.loc[ : , (\"Adj Close\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_fractions = \\\n",
    "    stocks \\\n",
    "    .isnull() \\\n",
    "    .mean() \\\n",
    "    .sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list =\\\n",
    "    sorted(list(missing_fractions\n",
    "                [missing_fractions > 0.3]\n",
    "                .index)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks1 =\\\n",
    "    stocks \\\n",
    "    .drop(labels= drop_list, \n",
    "          axis=1)\n",
    "stocks1 = stocks1.fillna(method = \"ffill\")\n",
    "stocks1 = stocks1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate average annual percentage return and volatilities over a theoretical one year period\n",
    "\n",
    "returns =\\\n",
    "(\n",
    "    stocks1\n",
    "    .pct_change()\n",
    "    .mean() \n",
    "    * 252\n",
    ")\n",
    "\n",
    "returns = pd.DataFrame(returns)\n",
    "\n",
    "returns.columns = [\"Returns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns[\"Volatility\"] =\\\n",
    "(    \n",
    "     stocks1\n",
    "    .pct_change()\n",
    "    .std() \n",
    "    * np.sqrt(252)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.asarray([np.asarray(returns['Returns']),np.asarray(returns['Volatility'])]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standarize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(data)\n",
    "rescaledDataset = pd.DataFrame(scaler.fit_transform(data),columns = returns.columns, index = returns.index)\n",
    "X = rescaledDataset\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, ward, fcluster\n",
    "#Calulate linkage\n",
    "Z = linkage(X, method = \"ward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = red> Answer 1 is presented in the cell below: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Dendogram\n",
    "\n",
    "plt.figure(figsize=(18, 10)\n",
    "          )\n",
    "plt.title(\"Stocks Dendograms\")\n",
    "\n",
    "dendrogram(Z, labels = X.index)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_threshold = 0.02\n",
    "\n",
    "clusters = fcluster(Z, distance_threshold, criterion='distance')\n",
    "\n",
    "chosen_clusters = pd.DataFrame(data=clusters, \n",
    "                               columns=['cluster']\n",
    "                              )\n",
    "\n",
    "chosen_clusters['cluster'].value_counts().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the dendrogram, which of the stocks are most correlated? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[chosen_clusters[chosen_clusters['cluster']==43].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also based on the dendrogram, please identify two stocks that are not well correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "randomly pick one from cluster1 and pick one from cluster2, they are not well correlated i.e.any different clusters(two largest clusters shown below)'s stock are not well correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_threshold = 10\n",
    "\n",
    "clusters = fcluster(Z, distance_threshold, criterion='distance')\n",
    "\n",
    "chosen_clusters = pd.DataFrame(data=clusters, \n",
    "                               columns=['cluster']\n",
    "                              )\n",
    "\n",
    "chosen_clusters['cluster'].value_counts().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[chosen_clusters[chosen_clusters['cluster']==1].index].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.iloc[chosen_clusters[chosen_clusters['cluster']==2].index].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëá <font color = \"purple\"> Bigger Question 2. ### \n",
    "\n",
    "### The second question asks you to run a principal components analysis (PCA) for portfolio management. Begin your analysis with all the above stocks. Make sure to employ the inclusion criterion of less than 30% of missing values.\n",
    "    \n",
    "    Your objective is to find the portfolio using PCA.\n",
    "    \n",
    "    Select and normalize the four largest components and use them as weights for \n",
    "    portfolios that you can compare to an equal-weighted portfolio comprising all stocks.\n",
    "    \n",
    "    Identify the profile of the portfolio based on the portfolio weights.\n",
    "    \n",
    "    When comparing the performance of each portfolio over the sample period \n",
    "    to \"the market\", assess the performance of other portfolios that capture different \n",
    "    return patterns.\n",
    "    \n",
    "> Please use 75% of your data for PCA and 25% for backtesting.    \n",
    "    \n",
    "### <font color = \"green\"> NOTE: The investment horizon will be 10 years between 2010 and 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the lines of code that lead to an answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = pdr.get_data_yahoo(list(nasdaq100_components[\"Ticker\"]), start=dt.datetime(2000,1,1), end=dt.datetime(2020,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = stocks.loc[:,(\"Adj Close\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values =\\\n",
    "(\n",
    "    stocks\n",
    "    .isnull() # True (1) vs. False (0)\n",
    "    .mean()\n",
    "    .sort_values(ascending = False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here don't know how to do, just drop columns which has missing_values\n",
    "drop_list =\\\n",
    "(\n",
    "    sorted(list(missing_values[missing_values > 0]\n",
    "                .index)\n",
    "          )\n",
    ")\n",
    "\n",
    "stocks =\\\n",
    "(\n",
    "    stocks\n",
    "    .drop(labels = drop_list,\n",
    "          axis = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks =\\\n",
    "(\n",
    "    stocks\n",
    "    .fillna(method = \"ffill\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Daily_Linear_Return =\\\n",
    "(\n",
    "    stocks\n",
    "    .pct_change(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop outlier\n",
    "Daily_Linear_Return =\\\n",
    "(\n",
    "    Daily_Linear_Return[Daily_Linear_Return \n",
    "                        .apply(lambda x:(x - x.mean() #by column\n",
    "                                        ).abs() < (3 * x.std()\n",
    "                                                  )\n",
    "                              )\n",
    "                        .all(1)#by row\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler =\\\n",
    "(\n",
    "    StandardScaler()\n",
    "    .fit(Daily_Linear_Return)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_stocks =\\\n",
    "(\n",
    "    pd\n",
    "    .DataFrame(scaler.fit_transform(Daily_Linear_Return),\n",
    "               columns = Daily_Linear_Return.columns,\n",
    "               index = Daily_Linear_Return.index)\n",
    ")\n",
    "\n",
    "scaled_stocks.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop =\\\n",
    "    int(len(scaled_stocks) * 0.75)\n",
    "\n",
    "X_Train = scaled_stocks[    : prop] \n",
    "X_Test  = scaled_stocks[prop:     ] \n",
    "\n",
    "X_Train_Raw = Daily_Linear_Return[    :prop]\n",
    "X_Test_Raw  = Daily_Linear_Return[prop:    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_tickers =\\\n",
    "(\n",
    " scaled_stocks\n",
    " .columns\n",
    " .values\n",
    ")\n",
    "\n",
    "stock_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "pca = PCA()\n",
    "\n",
    "PrincipalComponent = pca.fit(X_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCWeights():\n",
    "\n",
    "    weights = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(pca.components_[:4])#here only for 4 largest\n",
    "                  ):\n",
    "        weights[\"weights_{}\".format(i)] = pca.components_[i] / sum(pca.components_[i]\n",
    "                                                                  )\n",
    "\n",
    "    weights = weights.values.T\n",
    "    return weights # Team, be careful with indentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = PCWeights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sharpe_ratio(ts_returns, periods_per_year = 252):\n",
    "\n",
    "    n_years = ts_returns.shape[0] / periods_per_year\n",
    "\n",
    "    annualized_return = np.power(np.prod(1 + ts_returns), (1 / n_years)\n",
    "                                ) - 1\n",
    "\n",
    "    annualized_vol = ts_returns.std() * np.sqrt(periods_per_year)\n",
    "\n",
    "    annualized_sharpe = annualized_return / annualized_vol\n",
    "\n",
    "    return annualized_return, annualized_vol, annualized_sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yet another gift\n",
    "\n",
    "def backtest_PCA_porfolios(eigen):\n",
    "\n",
    "    eigen_prtfi =\\\n",
    "        (\n",
    "            pd\n",
    "            .DataFrame(data = {\"weights\": eigen.squeeze()\n",
    "                              },\n",
    "                       index = stock_tickers)\n",
    "        )\n",
    "\n",
    "    eigen_prtfi.sort_values(by = [\"weights\"],\n",
    "                            ascending = False,\n",
    "                            inplace = True)\n",
    "\n",
    "    eigen_prtfi_returns =\\\n",
    "    (\n",
    "        np\n",
    "        .dot(X_Test_Raw\n",
    "             .loc[ : , eigen_prtfi.index],\n",
    "             eigen)\n",
    "    )\n",
    "\n",
    "    eigen_portfolio_returns =\\\n",
    "    (\n",
    "        pd\n",
    "        .Series(eigen_prtfi_returns.squeeze(),\n",
    "                index = X_Test_Raw.index)\n",
    "    )\n",
    "\n",
    "    returns, vol, sharpe = calculate_sharpe_ratio(eigen_portfolio_returns)\n",
    "\n",
    "    print(\"Our PCA-based Portfolio:\\nReturn = %.2f%%\\nVolatility = %.2f%%\\nSharpe = %.2f\"  %\n",
    "          (returns * 100, vol * 100, sharpe)\n",
    "         )\n",
    "\n",
    "    # Compared with what? Equal-weightage Portfolio\n",
    "\n",
    "    equal_weight_return =\\\n",
    "    (\n",
    "        X_Test_Raw * (1 / len(pca.components_)\n",
    "                     )\n",
    "    ).sum(axis = 1)\n",
    "\n",
    "    df_plot =\\\n",
    "        (\n",
    "            pd\n",
    "            .DataFrame({\"ML Portfolio Return\": eigen_portfolio_returns,\n",
    "                        \"Equal Weight Index\": equal_weight_return},\n",
    "                      index = X_Test.index\n",
    "                      )\n",
    "        )\n",
    "\n",
    "    (\n",
    "        np\n",
    "        .cumprod(df_plot + 1)\n",
    "        .plot(title = \"Returns of the equal weighted index vs. Eigen-Portfolio\",\n",
    "              figsize = [16, 8]\n",
    "             )\n",
    "    )\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = red> Answer 2 is presented in the cell below: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(weights)):\n",
    "    backtest_PCA_porfolios(eigen = weights[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> üëâ Questions 3. Using `pandas.datareader`, extract the stock prices of the following ticker symbols, between July 2015 and June 2019.\n",
    "\n",
    "- General Motors `GM`\n",
    "- Marriott `MAR`\n",
    "- Pfizer `PFE`\n",
    "- ExxonMobil `XOM`\n",
    "- The Walt Disney Company `DIS`\n",
    "- Bank of America `BAC`\n",
    "- Proctor & Gamble `PG`\n",
    "- Hilton `HLT`\n",
    "- Walmart `WMT`\n",
    "- Twitter `TWTR`\n",
    "\n",
    "### Then, calculate simple daily percentage changes in the stock prices, and store them into an object, printing the results into an output cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the lines of code that lead to an answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_tickers = [\"GM\", \"MAR\", \"PFE\", \"XOM\", \"DIS\", \"BAC\", \"PG\", \"HLT\", \"WMT\", \"TWTR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = pdr.get_data_yahoo(stock_tickers, start=dt.datetime(2015,7,1),end=dt.datetime(2019,7,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = stocks.loc[:,(\"Adj Close\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = stocks.drop(labels = [\"TWTR\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = stocks.pct_change()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = red> Answer 3 is presented in the cell below: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> üëâ Questions 4. Using a box-and-whisker plot, compare the performance of the stocks over the given period of time. Find the stock with the highest variability and risk, based on the visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the lines of code that lead to an answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = red> Answer 4 is presented in the cell below: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[16,8])\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(returns)\n",
    "ax.set_xticklabels(returns.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> üëâ Questions 5. Create your own function to compare daily percentage changes between stocks, using a scatter plot and its distribution relative to a perfect diagonal (regression line). \n",
    "\n",
    "### Assess which of the following pairs seem to show the closest relationships.\n",
    "\n",
    "1. ExxonMobil (`XOM`) and General Motors (`GM`)\n",
    "2. Twitter (`TWTR`) and The Walt Disney Company (`DIS`)\n",
    "3. Marriott (`MAR`) and Hilton (`HLT`)\n",
    "4. Pfeizer (`PFE`) and Proctor & Gamble (`PG`)\n",
    "5. Bank of America (`BAC`) and Walmart (`WMT`)\n",
    "\n",
    "### Upon completion of the above, please execute more tasks for the sake of this question. \n",
    "\n",
    "### As you have learned in class, if you wish to look for all combinations of stocks you can use the scatter matrix graph provided by the `pandas` module. Create the scatter matrix, along with a Kernel Density Estimation on the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the lines of code that lead to an answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_stocks(stock1, stock2):\n",
    "    stocks = pd.DataFrame({stock1.name:stock1, stock2.name:stock2})\n",
    "    x = np.linspace(stocks.values.min(), stocks.values.max())\n",
    "    plt.plot(x,x)\n",
    "    plt.scatter(stock1, stock2)\n",
    "    \n",
    "    plt.xlabel(stock1.name)\n",
    "    plt.ylabel(stock2.name)\n",
    "    plt.title(f\"{stock1.name} vs {stock2.name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = red> Answer 5 is presented in the cell below: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_pairs = [[\"XOM\",\"GM\"], [\"MAR\", \"HLT\"], [\"PFE\", \"PG\"], [\"BAC\", \"WMT\"]]\n",
    "\n",
    "for pair in lst_pairs:\n",
    "    compare_two_stocks(returns[pair[0]], returns[pair[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess `MAR` and `HLT` seem to show the closest relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(returns,\n",
    "               figsize = (16, 16)\n",
    "              )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> üëâ Question 6. It is often useful to analyze stock performance against a market index such as the S&P 500. This will give a sense of how a stock price compares to movements in the overall market.\n",
    "\n",
    "### Carry out the following analysis steps.\n",
    "\n",
    "<font color = green>\n",
    "\n",
    "> ### 1. Extract the S&P 500 (`^GSPC`) data for the same time period used for the stocks in Question 1.\n",
    "\n",
    "> ### 2. In order to perform comparisons, you must run the same calculations to derive the daily percentage changes and cumulative returns on the index. You might first want to concatenate the index calculations in the results of the calculations of the stocks with respect to daily percentage changes. The process will lead you to efficiently compare the overall set of stocks and index calculations for daily percentage changes.\n",
    "\n",
    "> ### 3. Calculate the cumulative daily returns.\n",
    "\n",
    "> ### 4. To complete this analysis, calculate the correlation of the daily percentage change values.\n",
    "\n",
    "> ### 5. Using location accessor, print only the correlational coefficients of each stock relative to the S&P 500, in descending order.\n",
    "\n",
    "</font> \n",
    "        \n",
    "### Which stock price moved in the most similar way to the S&P 500? Which moved in the least similar way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the lines of code that lead to an answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500 = pdr.get_data_yahoo([\"^GSPC\"], start=dt.datetime(2000,1,1))[[\"Adj Close\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500 = sp500.rename(columns={\"Adj Close\":\"sp500\"})\n",
    "prices = pd.concat([sp500, stocks1], axis=1)\n",
    "returns = prices.pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cul_returns = np.cumprod(returns+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = returns.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = red> Answer 6 is presented in the cell below: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix[\"sp500\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which stock price moved in the most similar way to the S&P 500?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix[\"sp500\"].sort_values(ascending=False).index[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which moved in the least similar way?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix[\"sp500\"].sort_values(ascending=False).index[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = blue> üëâ Question 7. One common type of data visualization in finance is a stock‚Äôs trading volume relative to its closing price.\n",
    "\n",
    "### Create a chart below after obtaining the data from Yahoo Finance!, using `pandas.datareader`. The target symbol is `AMZN`, and our period of interest is between January 2007 and December 2009. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMZN = pdr.get_data_yahoo([\"AMZN\"], start=dt.datetime(2007,1,1), end=dt.datetime(2010,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the lines of code that lead to an answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visulize_price_volume(stock):\n",
    "    plt.figure(figsize=(18, 9))\n",
    "\n",
    "    plt.bar(stock.index, stock[\"Volume\"], color='blue', alpha=0.5)\n",
    "\n",
    "    ax2 = plt.twinx()\n",
    "\n",
    "    ax2.plot(stock.index, stock[\"Close\"], color='red')\n",
    "\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Volume')\n",
    "    ax2.set_ylabel('Closing Price')\n",
    "\n",
    "    plt.title('Stock Trading Volume vs. Closing Price')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = red> Answer 7 is presented in the cell below: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "visulize_price_volume(AMZN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëá <font color = \"purple\"> Bigger Question 8. \n",
    "\n",
    "### Please create a predictive model for the weekly return of NFLX stock. You will use supervised learning for your predictive modelling.\n",
    "\n",
    "> As you learned in class, to do this it is essential to know what factors are related to Netflix‚Äôs stock price, and to incorporate as much information as you can into the model.\n",
    "\n",
    "> Among the three major factors (correlated assets, technical indicators, and fundamental analysis), you will use correlated assets and technical indicators as features here.\n",
    "\n",
    "    Step 1. Use 75% of your data for the training of your algorithm, and 25% for the testing set.\n",
    "\n",
    "    Step 2. For your feature engineering...\n",
    "    \n",
    "> Our operational definition of `outcome` (`Y`) is the weekly return of Netflix (NFLX). The number of trading days in a week is assumed to be five, and we compute the return using five trading days. \n",
    "<br>\n",
    "    \n",
    "* <font color = \"green\"> NOTE: The lagged five-day variables embed the time series component by using a time-delay approach, where the lagged variable is included as one of the predictor variables. This step translates the time series data into a supervised regression-based model framework.\n",
    "<br>    \n",
    "    \n",
    "> For `input features` (`predictors`; `Xs`), we use (The variables used as predictors are as follows) ...\n",
    "\n",
    "> `Correlated assets`\n",
    "\n",
    "* lagged five-day returns of stocks (META, APPLE, AMZN, GOOGL);\n",
    "* currency exchange rates (USD/JPY and GBP/USD);\n",
    "* indices (S&P 500, Dow Jones, and VIX);\n",
    "* lagged five-day, 15-day, 30-day, and 60-day returns of NFLX.\n",
    "\n",
    "> `Technical indicators`\n",
    "\n",
    "* 21-day, 63-day, and 252-day moving averages;\n",
    "* 10-day, 30-day, and 200-day exponential moving averages;\n",
    "* 10-day, 30-day, and 200-day relative strength index;\n",
    "* stochastic oscillator %K and %D (using rolling windows of 10-, 30-, 200-day);\n",
    "* rate of change (using 10-, 30-day past prices).\n",
    "    \n",
    "    \n",
    "    Step 3. For your algorithm of choices, please assess the model performance of the following algorithms: \n",
    "\n",
    "    \n",
    "* Linear Regression\n",
    "* Elastic Net\n",
    "* LASSO\n",
    "* Support Vector Machine\n",
    "* K-Nearest Neighbor\n",
    "* ARIMA\n",
    "* Decision Tree\n",
    "* Extra Trees \n",
    "* Random Forest\n",
    "* Gradient Boosting Tree\n",
    "* Adaptive Boosting\n",
    "    \n",
    "    \n",
    "    Step 4. For this exercise, hyperparameter tuning is not requested. \n",
    "    \n",
    "    Step 5. But make sure to compare the model performance of the above algorithms.\n",
    "\n",
    "> The metric for assessing model performance will be mean squared error (`MSE`).\n",
    "<br>\n",
    "\n",
    "> Show which of the algorithms perform relatively better by a comparison visualization of performance, for both the training and testing sets learned in class. \n",
    "\n",
    "    Step 6. Using the model of your choice, please visualize the actual vs. predicted (estimated) data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the lines of code that lead to an answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_ticker = [\"NFLX\",\"META\",\"AAPL\",\"AMZN\",\"GOOGL\"]\n",
    "\n",
    "currency_ticker = [\"DEXJPUS\", \"DEXUSUK\"]\n",
    "\n",
    "index_ticker = [\"SP500\", \"DJIA\", \"VIXCLS\"]\n",
    "\n",
    "\n",
    "stock_data = pdr.get_data_yahoo(stock_ticker)\n",
    "currency_data = pdr.get_data_fred(currency_ticker)\n",
    "index_data = pdr.get_data_fred(index_ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_period = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y =\\\n",
    "    (np\n",
    "     .log(stock_data.loc[ : , (\"Adj Close\", \"NFLX\")]\n",
    "         )\n",
    "     .diff(return_period)\n",
    "    )\n",
    "\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.name = Y.name[-1]+\"_pred\"\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 =\\\n",
    "    (np.\n",
    "     log(stock_data.loc[ : , (\"Adj Close\", (\"META\",\"AAPL\",\"AMZN\",\"GOOGL\")\n",
    "                             )\n",
    "                       ]\n",
    "        )\n",
    "     .diff(return_period)\n",
    "     .shift(return_period)\n",
    "    )\n",
    "\n",
    "X1.columns =\\\n",
    "    (X1\n",
    "     .columns\n",
    "     .droplevel()\n",
    "    )\n",
    "\n",
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 =\\\n",
    "    (np\n",
    "     .log(currency_data)\n",
    "     .diff(return_period)\n",
    "    )\n",
    "\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 =\\\n",
    "    (np\n",
    "     .log(index_data)\n",
    "     .diff(return_period)\n",
    "    )\n",
    "\n",
    "X3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 =\\\n",
    "    (\n",
    "    pd\n",
    "    .concat([np\n",
    "             .log(stock_data.loc[ : , (\"Adj Close\", \"NFLX\")\n",
    "                                ]\n",
    "                 )\n",
    "             .diff(i) for i in [return_period, \n",
    "                                return_period * 3, \n",
    "                                return_period * 6, \n",
    "                                return_period * 12]\n",
    "            ],\n",
    "           axis = 1\n",
    "           )\n",
    "    .shift(return_period)\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "X4.columns = [\"NFLX_DT\", \"NFLX_3DT\", \"NFLX_6DT\", \"NFLX_12DT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFLX = pdr.get_data_yahoo([\"NFLX\"])\n",
    "NFLX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X5 = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            NFLX[\"Adj Close\"]\n",
    "            .rolling(i)\n",
    "            .mean() \n",
    "            for i in [21\n",
    "                      ,63\n",
    "                      ,252\n",
    "                     ]\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "X5.columns = [\"NFLX_SMA21\",\"NFLX_SMA63\",\"NFLX_SMA252\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X6 = (\n",
    "    pd.concat(\n",
    "        [\n",
    "            NFLX[\"Adj Close\"]\n",
    "            .ewm(i).mean() \n",
    "            for i in [10,\n",
    "                      30,\n",
    "                      200]\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "X6.columns = [\"NFLX_EMA10\",\"NFLX_EMA30\",\"NFLX_EMA200\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_RSI(period, stock1):\n",
    "    stock = stock1.copy()\n",
    "    stock[\"change\"] = stock.diff()\n",
    "\n",
    "    stock[\"gain\"] = stock[\"change\"].apply(lambda x: x if x > 0 else 0)\n",
    "    stock[\"loss\"] = stock[\"change\"].apply(lambda x: -x if x < 0 else 0)\n",
    "\n",
    "    stock[\"avg_gain\"] = stock[\"gain\"].rolling(period).mean()\n",
    "    stock[\"avg_loss\"] = stock[\"loss\"].rolling(period).mean()\n",
    "    \n",
    "    for i in range(period, len(stock)):\n",
    "        stock.iloc[i,stock.columns.get_loc(\"avg_gain\")] = (stock.iloc[i-1][\"avg_gain\"]*13 + stock.iloc[i][\"gain\"])/14\n",
    "        stock.iloc[i,stock.columns.get_loc(\"avg_loss\")] = (stock.iloc[i-1][\"avg_loss\"]*13 + stock.iloc[i][\"loss\"])/14\n",
    "    \n",
    "    stock[\"RS\"] = stock[\"avg_gain\"]/stock[\"avg_loss\"]\n",
    "    stock[\"RSI\"] = 100 - 100/(1+stock[\"RS\"])\n",
    "    \n",
    "    return stock[\"RSI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X7 = pd.DataFrame([cal_RSI(10,NFLX[[\"Adj Close\"]]),cal_RSI(30,NFLX[[\"Adj Close\"]]),cal_RSI(200,NFLX[[\"Adj Close\"]])]).T\n",
    "\n",
    "X7.columns = [\"NFLX_RSI10\",\"NFLX_RSI30\",\"NFLX_RSI200\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_SO(period, stock1):\n",
    "    stock = stock1.copy()\n",
    "    stock[\"lowest_low\"] = stock[\"Low\"].rolling(period).min()\n",
    "    stock[\"highest_high\"] = stock[\"High\"].rolling(period).max()\n",
    "    stock[f\"{period}%K\"] = ((stock[\"Close\"] - stock[\"lowest_low\"]) / (stock[\"highest_high\"] - stock[\"lowest_low\"])) * 100\n",
    "    stock[f\"{period}%D\"] = stock[f\"{period}%K\"].rolling(3).mean()\n",
    "    \n",
    "    return stock[[f\"{period}%K\", f\"{period}%D\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X8 = pd.concat([cal_SO(10, NFLX),cal_SO(30, NFLX),cal_SO(200, NFLX)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X9 = pd.concat([NFLX[\"Adj Close\"].pct_change(10)*100, NFLX[\"Adj Close\"].pct_change(30)*100], axis = 1)\n",
    "X9.columns = [\"ROC10\", \"ROC30\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X1, X2, X3, X4, X5, X6, X7, X8, X9],axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =\\\n",
    "(\n",
    "pd\n",
    ".concat([Y, X],\n",
    "        axis = 1)\n",
    ".dropna()\n",
    ".iloc[ : :return_period, :]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data.loc[ : , Y.name]\n",
    "\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[ : , X.columns]\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 0.25\n",
    "\n",
    "train_size =\\\n",
    "    int(len(X) \n",
    "        * \n",
    "        (1 - validation_size)\n",
    "       )\n",
    "\n",
    "X_train, X_test =\\\n",
    "    (X[0         :train_size], \n",
    "     X[train_size:len(X)    ]\n",
    "    )\n",
    "\n",
    "Y_train, Y_test =\\\n",
    "    (Y[0         :train_size], \n",
    "     Y[train_size:len(X)    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next we fill in the model mentioned in step 3\n",
    "# Loading Algorithm\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Regularization\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# ENSEMBLE\n",
    "\n",
    "## Bagging\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "## Boosting\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Support Vector Machine\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# K-Nearest Neighbor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Multi-layer Perceptron (Neural Networks)\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for assessment\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#for ingore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "#Linear Regression\n",
    "models.append((\"LR\", LinearRegression()\n",
    "             )\n",
    "            )\n",
    "\n",
    "#Elastic Net\n",
    "models.append((\"EN\", ElasticNet()\n",
    "             )\n",
    "            )\n",
    "\n",
    "#LASSO\n",
    "models.append((\"LASSO\", Lasso()\n",
    "             )\n",
    "            )\n",
    "\n",
    "#Support Vector Machine\n",
    "models.append((\"SVR\", SVR()\n",
    "             )\n",
    "            )\n",
    "\n",
    "#K-Nearest Neighbor\n",
    "models.append((\"KNN\", KNeighborsRegressor()\n",
    "             )\n",
    "            )\n",
    "\n",
    "#ARIMA\n",
    "\n",
    "#Decision Tree\n",
    "models.append((\"CART\", DecisionTreeRegressor()\n",
    "             )\n",
    "            )\n",
    "\n",
    "#Extra Trees\n",
    "models.append((\"ETR\", ExtraTreesRegressor()\n",
    "              )\n",
    "             )\n",
    "\n",
    "#Random Forest\n",
    "models.append((\"RFR\", RandomForestRegressor()\n",
    "              )\n",
    "             )\n",
    "\n",
    "#Gradient Boosting Tree\n",
    "models.append((\"GBR\", GradientBoostingRegressor()\n",
    "              )\n",
    "             )\n",
    "\n",
    "#Adaptive Boosting\n",
    "models.append((\"ABR\", AdaBoostRegressor()\n",
    "              )\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    \n",
    "    names.append(name)\n",
    "    \n",
    "    res = model.fit(X_train, Y_train)\n",
    "    train_result = mean_squared_error(res.predict(X_train), Y_train)\n",
    "    train_results.append(train_result)\n",
    "    \n",
    "    test_result = mean_squared_error(res.predict(X_test), Y_test)\n",
    "    test_results.append(test_result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we're not done yet still have ARIMA left\n",
    "import statsmodels.tsa.arima.model as stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "modelARIMA =\\\n",
    "(    stats\n",
    "     .ARIMA(endog = Y_train,\n",
    "                exog = X_train,\n",
    "                order = [1, 0, 0]\n",
    "            )\n",
    ")\n",
    "\n",
    "model_fit = modelARIMA.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = mean_squared_error(Y_train,model_fit.fittedvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted =\\\n",
    "(\n",
    "    model_fit\n",
    "    .predict(start = train_size - 1,\n",
    "             end = len(X) - 1,\n",
    "             exog = X_test)[1: ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = mean_squared_error(Y_test,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "names.append(\"ARIMA\")\n",
    "train_results.append(train_mse)\n",
    "test_results.append(test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally we visualize it\n",
    "fig = plt.figure(figsize = [16, 8])\n",
    "\n",
    "ind = np.arange(len(names)\n",
    "               )\n",
    "\n",
    "width = 0.30\n",
    "\n",
    "fig.suptitle(\"Comparing the Perfomance of Various Algorithms on the Training vs. Testing Data\")\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "(plt\n",
    " .bar(ind - width/2,\n",
    "      train_results,\n",
    "      width = width,\n",
    "      label = \"Errors in Training Set\")\n",
    ")\n",
    "\n",
    "(plt\n",
    " .bar(ind + width/2,\n",
    "      test_results,\n",
    "      width = width,\n",
    "      label = \"Errors in Testing Set\")\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(names)\n",
    "\n",
    "plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = models[-1][1].fit(X_train,Y_train)\n",
    "predicted = res.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pd.DataFrame(predicted)\n",
    "predicted.index = Y_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color = red> Answer 8 is presented in the cell below: </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16, 8])\n",
    "plt.plot(predicted, label=\"ABR_predicted_data\")\n",
    "plt.plot(Y_test, label=\"actual data\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üíØ ‚ÄúThank you for putting your efforts into the individual assessment questions‚Äù üòä"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
